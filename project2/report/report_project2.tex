\documentclass{article}
\title{Project 2 \\ FYS-STK3155/4155}
\author{Magnus Bergkvam}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}
\usepackage{listings}


\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
In this project we tested out performances of neural networks for regression and
binary classification purposes, and compared this with the linear models we
explored in the previous project \cite{githubrepoproject1} (in the case of
regression), as well as our own implementation of logistic regression (for
classification). We here implemented our own neural network code from scratch,
basing the training on the backpropagation-algorithm.  With gradient
optimization being a central part of the development of both neural networks and
logistic regression we also dove into various ways of optimizing the cost
function through the Newton-Raphson method, in particular. We also explored what
effect the various hyperparameters introduced in the methods have on how quick we
are able to optimize the cost, and the performance of the final model. In
particular we explored the effect of the learning-rate and
regularization-parameter, and we looked at various methods for optimizing
through the Newton-Raphson method, including ordinary gradient descent,
stochastic gradient descent, adagrad, rmsprop and adam. For the regression we
used the franke-function which we introduced in project 1
\cite{githubrepoproject1}, while for classification we used breast cancer data
from wisconsin \cite{sklearncancerdata} \cite{breastcancerwisconsin}. What we
saw was that for the franke-function with regularization we did not get any
major performance gains compared to the simpler linear models we tested in
project 1 \cite{githubrepoproject1}. For the cancer data we also saw that while
the neural networks did perform a little better than logistic regression,
however the logistic regression also managed to give us very good results.
Throughout the project we have manifested again that just because neural
networks might be very powerful and impressive in certain tasks, there are still
cases where simpler models will outperform them. We also saw which parts of a
neural net was most instrumental for good model performance, with especially the
learning-rate, regularization parameter, the learning-rate scheduler, and the
amount of epochs being the most instrumental for cost-minimization, while
activation functions and hidden layer sizes didn't matter all that much.

\section{Introduction}
In the last $10$ or so years, neural networks have really exploded in
popularity. Various different types of neural networks have proved effective for
various different applications all from speech-recognition, to image processing,
to complex language tasks. But are they always the best option, and how do we
get the most performance out of them?

These are some of the things which we aim to answer here. To do this we will
implement a rather simple multi-layer perceptron model from scratch using the
backpropagation algorithm. We will then use this implementation in a
regression-setting, generating data from the franke-function as we did in
project $1$, and of course compare this to the results we got with the linear
models we explored in project $1$. We will also use our neural network
implementation in a classification setting, where we are trying to classify
whether a patient has cancer or not based on various medical features.
Additionally we will explore various methods of gradient optimizations, as both
the methods we will deal with in this project (that is neural networks and
logistic regression) have cost-functions which one would typically solve using
some sort of gradient method.  As already mentioned we will implement and
explore performances based on plain gradient descent, stochastic gradient
descent, adagrad, rmsprop and adam. For all our different methods we will
compare our own implementations against established libraries, namely tensorflow
and scikit-learn.

There are mainly two reasons why we are specifically focusing on using neural
networks in this project. One is, as already mentioned, that they have become
very popular in the last decade or so, with them being used for more and more
tasks. With their usage increasing it becomes important to understand the way a
model works, and how we get the most amount of performance out of them by
tweaking the various different parts of the network. Additionally since they, in
comparison to a lot of other models, are quite computationally expensive it is
important to realize which of the parameters are the most relevant to tweak, and
which are not so relevant as this can save a lot of time. Another reason is
because of the universal approximation theorem applying to neural networks. We
will go more deeply into this in \ref{univ-approx-thm}, but the essence is that
a sufficiently big neural network should be able approximate any function
multidimensional function to an arbitrary accuracy
\cite[s.~13.5]{lecutenotes13}. We will see that this doesn't necessarily
guarantee us a good model.

\section{Methods}
\subsection{The franke function}
The franke-function is a $2$-dimensional function which we already have explored
in project $1$ \cite{githubrepoproject1}, so I won't go too deeply into this
here. The function is given by:
\begin{align*}
      f(x,y) & = \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
             & +\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}
Again we will draw 200 of random observations $\bm{x}_1$ and $\bm{x}_2$, with
$(x_1)_i \in [0, 1]$, and $(x_2)_i \in [0, 1]$ for $1 \leq i \leq 200$. The response will be generated by:
$$\bm{y} = f(\bm{x}_1, \bm{x}_2) + \bm{\epsilon}$$
, with $\epsilon_i \sim N(0, 0.1^2)$ (we apply $f$ element-wise). We will try to
fit a neural network to this data with this time only $\bm{x}_1$ and $\bm{x}_2$
as the response. In project $1$ we added polynomial features of various degrees
to fit to a model. Here we do not need to do this as we can just add enough
nodes and the universal approximation theorem assures us that the network should
do a good job of approximating the function.

\subsection{The wisconsin cancer data}
The wisconsin cancer data \cite{breastcancerwisconsin} is a dataset containing
breast cancer cases, with a response-variable indicating wether the patient has
breast-cancer or not, with some corresponding medical features. This is clearly
a binary classification case, which makes this suitable for both neural networks
and logistic regression models. We will use the dataset included in
\textit{sklearn.datasets} \cite{sklearncancerdata}. Loading it into python and
splitting into train, validation and test we do by:

\begin{lstlisting}
X, y = load_breast_cancer(return_X_y=True)
y = y.reshape(len(y), 1)

# Train-test-validation splitting
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.4)
\end{lstlisting}

We split into train, test and validation sets because we will evaluate a lot of
models, and then it becomes increasingly important to evaluate the final
selected model on some test set in order for us to get a reliable final metric.
The data is limited to $569$ observations with each having $30$ explanatory
variables. This makes both the validation set and test set rather small, so with
around $100$ and $70$ observations in each respectively. We then need to be a
little skeptical towards the final metrics, as the metrics may be a little
specific to the split of data.

\subsection{The Newton-Raphson method}
Very central to the numerical optimization we do is the Newton-Raphson method.
This is a method which allows us to find the minimum of some function by using
its gradient. The method has it's origins in the Taylor-expansion of the
function we wish to minimize. In our case we use this to minimize cost-functions
for the models we use. Let $C(\bm{\beta}; \bm{X}, \bm{y})$ be some general
cost-function depending on some design matrix $\bm{X}$ and a response vector
$\bm{y}$. We wish to find the $\bm{\beta}$ which minimizes this cost, given
$\bm{X}$ and $\bm{y}$, i.e. we minimize $C$ with respect to $\bm{\beta}$, while
$\bm{X}$ and $\bm{y}$ are obviously fixed. We can then approximate the cost using a
Taylor-expansion of second order around some value $\bm{\beta}^*$, in which case
we get:
\begin{align*}
      C(\bm{\beta}, \bm{X}, \bm{y}) & \approx C(\bm{\beta}^*; \bm{X}, \bm{y}) + \frac{\partial C(\bm{\beta}^*; \bm{X}, \bm{y})}{\partial \bm{\beta}}(\bm{\beta} - \bm{\beta}^*) + \frac{1}{2} \frac{\partial^2 C(\bm{\beta}; \bm{X}, \bm{y})}{\partial^2 \bm{\beta}}(\bm{\beta} - \bm{\beta}^*)^2 \\
                                    & = C(\bm{\beta}^*; \bm{X}, \bm{y}) + \nabla_{\bm{\beta}} C(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) + \frac{1}{2} \bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*)^2
\end{align*}
Where $\nabla_{\bm{\beta}} C$ is the gradient of $C$ w.r.t. $\bm{\beta}$ and
$\bm{H}_{\bm{\beta}}$ is the Hesse-matrix (second derivative) of $C$ w.r.t.
$\bm{\beta}$. This should be a good approximation for $C$ sufficiently close to
$\bm{\beta}^*$. We then can find the derivative of this w.r.t. $\bm{\beta}$:
\begin{align*}
      \frac{\partial C(\bm{\beta}, \bm{X}, \bm{y})}{\partial \bm{\beta}} & \approx
      \nabla_{\bm{\beta}} C(\bm{\beta}^*) + \bm{H}_{\bm{\beta}}(\bm{\beta}^*)(\bm{\beta} - \bm{\beta}^*) \\
\end{align*}
If we set this to $0$ we further get:
$$\bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) = -\nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
which leads to
$$\bm{\beta} - \bm{\beta}^* = - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
and finally
$$\bm{\beta} = \bm{\beta}^* - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$

This tells us that the minimum of the Taylor-expansion of the cost-function,
using any $\bm{\beta}^*$ is given as above. The problem with this is that this
is using the Taylor-expansion, which is only reliable close to $\bm{\beta}^*$ so
chances are that using this formula will not give us a global minimum. However
it generally tends to bring us closer to the minimum we are looking after. What
we then usually do is to start with some initial guess for the optimal
$\bm{\beta}$, which we denote $\bm{\beta}^{(0)}$. We then for $k=1, \dots$ let
$\bm{\beta}^{(k)} = \bm{\beta}^{(k-1)} -
      \bm{H}_{\bm{\beta}}(\bm{\beta}^{(k-1)})^{-1} \nabla_{\bm{\beta}}
      C(\bm{\beta}^{(k-1)})$ all the way until we have convergence.

\subsection{Learning-rate schedulers}
Something which often is problematic when using the Newton-Raphson method as
described over is that we need to estimate the Hessian matrix and also invert
it. First of all finding a way to calculate the second derivative can be a
difficulty in and of itself, but it may also be computationally expensive to do
so. Not only that, we also need to invert this matrix which can also become very
computationally expensive, and perhaps also numerically unstable with higher
dimensional hesse matrices (remember that the Hessian matrix is a square matrix
with the same dimensions as $\bm{\beta}$ in both width and height).  Inverting
this matrix then may not be a problem if we only have a few parameters, but for
example when dealing with logistic regression with lots of features, or neural
networks with lots of hidden nodes and layers this becomes unviable. We
therefore will use various methods to approximate this hessian matrix.

\subsubsection{Ordinary gradient descent}
Ordinary gradient descent is a very simple way of approximating the inverse of
the Hessian matrix. Here we simply approximate it with some numerical
learning-rate, which we denote $\eta$. This way our algorithm becomes:
$$\bm{\beta}^{(k)} = \bm{\beta}^{(k-1)} - \eta \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)}).$$
Often we may just keep this fixed, but we can also make it depend on $k$. For
example one may wish to start with a higher learning-rate and make this decrease
with the number of iterations. This method I will refer to as using time-decay.
When using gradient descent for training our model $\eta$ then becomes a
hyperparameter in our model. Often this hyperparameter is rather important in
determining the fit of the model. We often want a rather high learning-rate as
this will lead to faster convergence towards some optimum, however if we make it
to big we may not get convergence at all. I will mainly throughout the code
simply set up a grid of learning-rates and fit to all of them, and simply choose
the learning-rate which gives the best performance, but there are countless
other more advanced way of tuning this parameter.

\subsubsection{Stochastic gradient descent}
Stochastic gradient descent is very similar to ordinary gradient descent, except
the fact that instead of training for each epoch on the whole dataset we split
the data into $n$ randomly chosen mini-batches each of some specified size (this
is what \textit{sgd\_size} in our code stands for). Then we do training over all
of the $n$ mini-batches for each epoch, while the rest of the method is the same.

\subsection{Momentum}
One problem with gradient descent and stochastic gradient descent which one
often can see is that it can be quite slow to learn. Ideally we would like the
model to minimize the cost/loss as quickly as possible, and therefore one might
want to use methods designed to accelerate this learning process. One relatively
simple such method is using a momentum, which often accelerates learning quite
drastically especially in cases with high curvatures and small consistent or
noisy gradients \cite[s.~8.3.2]{goodfellow2016deep}. This method introduces a
variable $\bm{v}$ which describes the direction and speed which the parameter
$\bm{\theta}$ is moving in parameter space \cite[s.~8.3.2]{goodfellow2016deep}, much
like the role of velocity in physics. To calculate this $\bm{v}$ we need a
hyperparameter $\alpha \in [0, 1)$ and we initialize $\bm{v} = \bm{0}$. Then for
each learning-iteration we update $\bm{v}$ the following way:
$$\bm{v} = \alpha \bm{v}_{prev} - \eta \Delta_{\bm{\theta}} C(\bm{x}, \bm{y}, \bm{\theta})$$
We then can simply update the $\bm{\theta}$ as follows:
$$\bm{\theta} = \bm{\theta}_{prev} + \bm{v}$$
We then see that the value of the $\bm{v}$ will depend on the current cost-gradient
w.r.t. $\bm{\theta}$ as well as the previous $\bm{v}$, which again depends on
the previous cost-gradient w.r.t. $\bm{\theta}$ and the previous previous
$\bm{v}$ and so on. Therefore we see that $\bm{v}$ is influenced by all the
previous cost-gradients, weighted differently, in contrast to just using the
current cost-gradient, and granted they point in a somewhat similar direction,
the $\bm{v}$ will grow bigger and bigger in size. It is therefore not that
difficult to see that this can give quicker learning.


Keep in mind that in theory there is nothing stopping us from combining momentum
with any of the other methods we discuss here, however methods like adam already
incorporates a form of momentum \cite[s.~8.5.3]{goodfellow2016deep}, so we might
not get much quicker learning by using it, if any at all.

\subsubsection{AdaGrad}
AdaGrad is another way designed to give faster learning. This is an algorithm
which uses adaptive learning-rates. These have a difference in that each axis of
the parameter-space $\bm{\theta}$ essentially get a separate learning-rate
\cite[s.~8.5]{goodfellow2016deep}, while these separate learning-rates change/adapt
throughout the learning process, which is a different approach to using momentum.
AdaGrad is a rather simple way of achieving a good such adaptive learning-rate.
It scales the parameters with with big updates, and scales up the ones with
small corresponding updates up. For AdaGrad we introduce one variable $\bm{r}$
which we initially set to $\bm{0}$, as well as a small constant $\delta$ which
we typically set to $10^{-7}$. The algorithms is as follows until the
stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\bm{r}_{prev} + \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta
                  \frac{1}{\delta + \sqrt{\bm{r}}} \odot \bm{g}$ and update
            $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).

\end{itemize}

\subsubsection{RMSProp}
RMSProp is much like AdaGrad, but is designed to be more robust for
cost-functions with very complex structures like neural nets. Unlike AdaGrad
where we shrink the learning-rates using the sizes of all the previous partial
derivatives of the cost-function for all the axis RMSProp focuses less on the
history of gradients from very long ago, but otherwise using much of the same
approach. The RMSProp algorithm requires therefore a decay-rate $\rho$ as a
hyperparameter as well, along the $\bm{r}$ which we again initialize to be
$\bm{0}$, as well as a small constant $\delta$, which typically is set to
$10^{-6}$. Then we do the following until the stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\rho \bm{r}_{prev} + (1 - \rho) \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta \frac{1}{\sqrt{\delta + \bm{r}}}
                  \odot \bm{g}$ and update $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).
\end{itemize}

\subsubsection{Adam}
Adam is a very popular learning-rate scheduler, which is probably the method we
will tackle which most often gives the best results. Our implementation is
essentially as described in algorithm 8.7 in \textit{Deep Learning}
\cite[s.~8.6.1]{goodfellow2016deep}. We start the algorithm by initializing the
parameter $\bm{\theta}$ which we would like to optimize the cost/loss w.r.t.
(typically we initialize this by drawing it randomly from for example the normal
distribution which is what we have done in this project). We also set beforehand
$\bm{s} = \bm{0}$, $\bm{r} = \bm{0}$ and $t=0$. Here the algorithms is as
follows until we meet the stopping criterion:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Increment $t$ by $1$ (i.e. set $t=t_{prev}+1$).
      \item Update $\bm{s}$ to become $\rho_1 \bm{s}_{prev} + (1 - \rho_1)\bm{g}$.
      \item Update $\bm{r}$ to become $\rho_2 \bm{r}_{prev} + (1 - \rho_2)g \odot
                  g$ ($\odot$ again being the Hadamard-product).
      \item Correct the bias in the first moment by: $\hat{\bm{s}} = \frac{\bm{s}}{1 - \rho_1^t}$.
      \item Correct the bias in the second moment by: $\hat{\bm{r}} = \frac{\bm{r}}{1 - \rho_2^t}$.
      \item Finally calculate $\Delta \bm{\theta} = -\eta
                  \frac{\hat{\bm{s}}}{\sqrt{\hat{\bm{r}}} + \delta}$ and update $\bm{\theta}$
            accordingly (i.e. set $\bm{\theta} = \bm{\theta}_{prev} + \Delta
                  \bm{\theta}$).
\end{itemize}
Where $\eta$ is the step-size (or learning-rate as we will call it), $\rho_1$
and $\rho_2$ are the decay-rate which must be in $[0, 1)$ (typically we set the
defaults to $\rho_1 = 0.9$ and $\rho_2 = 0.999$), $\delta$ is just a small
constant (we set it to $10^{-8}$). Note that we can in theory drop the minibatch
sampling and still get a functioning algorithm, but this will often give much
worse results.

\subsection{Automatic differentiation}
What is clear is that in our optimization it will be central to calculate the
gradient of some loss function with respect to some parameter, however it is not
always easy to find analytical gradients by hand. Automatic differentiation is
an algorithmic approach to calculating the gradients of functions. We will take
some usage of this in our code through the python-library jax
\cite{githubrepojax}, but this is a somewhat minor part of the project so I
won't go too much into detail about this. This utilizes clever applications of
the chain rule in order to calculate the gradients of functions without
compromising on accuracy (it usually gives almost exactly the same results as an
analytical expression).  We will utilize this in some of our optimizations
instead of calculating gradients by hand, and for adding support in our code so
that users don't have to manually calculate gradients by hand, which can be
time-consuming, and a potential for errors in the calculations.

\subsection{Neural networks}
There are many different implementations of neural networks out there. We will
in this project focus on one of the simplest ones to implement, namely the
multi-layer-perceptron. The multi-layer perceptron consists of a input layer
with $p$ nodes (granted we have $p$ covariates), as many hidden layers as we
want, each of which can have also as many nodes as we want, and then an output
layer of the same dimensionality as our response. Linking the layers together is
a set of weights, where each node in the layer before is connected to each node
in the next layer. Additionally each node has it's bias we add in for each node.
This is then sent in to an activation function, which is set for each layer. The
output of this becomes the values (also called activations) of the neurons in
the next layer. Typical choices of activation functions are sigmoid, relu or
leaky relu, which are the ones we will explore in this project. For the final
layer we instead call this the output function. The output function is typically
tailored to whichever data we have. If we for example want to do regression we
often just set this to the identity-function, i.e. we do not transform the
activations at all. In the case where we are doing classification we this choice
is no longer a good one as the identity function is not bounded, and we in that
case want the output to be a probability between $0$ and $1$, so then a
sigmoid-function for example is a good choice.

\subsubsection{Forward propagation algorithm}
With the description of the model done the forward propagation is actually
rather simple. We assume we have $L-1$ hidden layer, with index $L$ being the output layer. Then the following

\subsubsection{Backward propagation algorithm}

\subsubsection{Weight and bias initialization}
We see that at the core of neural networks are these weigh-matrices and bias
vectors, and seeing as we want to use an iterative method (namely the
Newton-Raphson method) it becomes clear that we need to initialize the weights
and biases to something. If we were to initialize all the weights/biases to be
$0$ (or some other constant), all the weights/biases would have the same effect
on the loss, and hence the backward-propagation algorithm. Our solution to this
is to draw the weights randomly from the standard-normal distribution, this way
we will be able to break the unwanted symmetry in our learning-process. For
biases one don't always bother initializing these randomly (although there is
nothing hindering us to do so). What we will do initialize all the biases to the
same value in every layer (we will set then to $0.1$). Setting it to a small
positive value will often activate the neurons when we are using a
relu-function (see \ref{relu-desc}), which again hinders the derivative of the
relu to be $0$, and this enables the network to learn
\cite[s.~6.3.1]{goodfellow2016deep}.

\subsubsection{The universal approximation theorem}
\label{univ-approx-thm}
The universal approximation theorem tells us that if we have a neural network
with one hidden layer and a non-linear activation-function in the hidden layer,
if we have enough nodes in the hidden layer we should be able to approximate any
continous multi-dimensional function to any accuracy
\cite[s.~13.5]{lecutenotes13}. Keep in mind that this is merely to the
training-data. A neural network can be able to approximate the training-data
very well, without necessarily getting that great performance for new data.

\subsection{Activation functions}
For this project we will explore three different activation functions for the
hidden layers.
\subsubsection{Sigmoid}
The sigmoid activation function limits the activations of the neurons between
$0$ and $1$. The sigmoid activation function is given as the following:
$$\sigma(x) = \frac{1}{1 + \exp(-x)}$$
With the derivative being (this is quite straightforward to calculate):
$$\sigma'(x) = \sigma(x) (1 - \sigma(x))$$
Additionally as mentioned this will also be used as an output-function when we
are doing binary classification, due to the fact that the final neuron
activations will be between $0$ and $1$.

\subsubsection{Relu}
\label{relu-desc}
Another popular activation function we will use is the relu activation function.
Unlike the sigmoid function this does not have a upper bound. Essentially the
idea is to. The function then can be expressed by:
$$\text{relu}(x) = \max(x, 0)$$
The derivative then becomes:
$$\text{relu}'(x) = \begin{cases}
            0 \qquad & \text{if } x < 0 \\
            1        & \text{if } x > 0
      \end{cases}$$
Keep in mind that the derivative is not defined for $x=0$. This however is
usually unproblematic as we almost never get activations being exactly $0$. In
our implementation we will just set the derivative to $0$ if the activation is
exactly $0$ (but again this should more or less never happen).

\subsubsection{Leaky relu}
The leaky relu is a generalization of the relu-function which unlike the
relu-function doesn't completely neutralize negative values.  Technically this
makes this function not bounded below, however with $\delta$ being rather small
severely limits the size of the negative activations, while no such shrinkage is
applied for the positive activations of course. The leaky relu is given by:
$$\text{lrelu}(x) = \begin{cases}
            x \qquad & \text{if } x > 0    \\
            \delta x & \text{if } x \leq 0
      \end{cases}$$
The derivative then becomes:
$$\text{lrelu}'(x) = \begin{cases}
            \delta \qquad & \text{if } x < 0 \\
            1             & \text{if } x > 0
      \end{cases}$$
We see one major difference between this and the relu is that for negative
activations, the derivative of this will not be $0$, but $\delta$. This means
that the network will be able to learn on activations which are negative as
well, albeit likely slower than the positive cases.

\subsection{Cost functions}
In this project we will use mainly two activation functions, namely mse/sse for
regression and cross-entropy for binary-classification. We will now quickly go
into these.

\subsubsection{MSE/SSE}
A natural choice of cost-function when dealing with regression tasks is the MSE.
When the target dimensions are always $1$ (which they are in this project), the
MSE is defined like:
$$\text{mse}(\bm{h}^{(L)}, \bm{t}) = \frac{1}{n} \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = \frac{1}{n}(\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
The derivative with respect to $\bm{h}^{(L)}$ when becomes:
$$\frac{\partial \text{mse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = \frac{2}{n} (\bm{h}^{(L)} - \bm{t})$$
We will also use the total sum of squares (SSE). Minimizing this is the same as
minimizing the MSE, but with one less floating-point operation. The SSE is given
by:
$$\text{sse}(\bm{h}^{(L)}, \bm{t}) = \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = (\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
and its derivative then becomes:
$$\frac{\partial \text{sse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = 2(\bm{h}^{(L)} - \bm{t})$$
The only potential problem with using SSE instead of MSE is that the gradient
will then be dependent on the amount of observations we are using, which can be
problematic with stochastic gradient descent. However in this project we will
set a fixed size of each minibatch, which guarantees that each training-set sent
into this cost/loss function will be of the same size, so this becomes
unproblematic.

\subsubsection{Cross-entropy}
For binary classification we will use cross-entropy as out cost-function.
Minimizing the cross-entropy is equivalent to maximizing the likelihood. We have
the cross-validation given by:
$$\text{cross-entropy}(\bm{h}^{(L)}, \bm{t}) = -\sum_{i=1}^{n} ( t_{i} \log(h^{(L)}_i) + (1 - y) \log(1 - h^{(L)}_i) )$$
With the gradient w.r.t. $h^{(L)}$ then becoming:
$$\frac{\partial \text{cross-entropy}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = (\bm{h}^{(L)} - \bm{t}) / ((1 - \bm{h}^{(L)}) \odot \bm{h}^{(L)})$$
Here the division is element-wise. One thing to note is that we see that if one of
our predictions $h^{(L)}_i$ is very close to $0$ while the target value is $1$
or vice versa, which especially can happen in the first iterations, perhaps the
most with some sort of unbounded activation function in the last layer, the
gradient can become very big. If we get too big of a gradient this can be
problematic for many reasons, with one being we can get values too big for the
computer to calculate. In our code we will solve this by clipping the
predictions in the cross-entropy-gradient, so that they don't become too close
to $0$ or $1$ so that this doesn't become a problem. Additionally we will add a
small $\delta$ inside the log of the cross-entropy for the same reason.

\subsubsection{Notation in the code}
Keep in mind that in the code the cost-functions often takes $\hat{y}$ and $y$
instead of $\bm{h}^{(L)}$ and $\bm{t}$ respectively. This essentially means the
same thing, but makes more sense with the notation for usage outside of neural
networks.

\subsection{Model complexities}
One final thing to consider which will be very important for our results is the
model-complexity of neural networks and logistic regression. Logistic regression
requires just as many parameters as we have explanatory variables, so the model
complexity is rather low, at least in the case where we don't have too many
covariates. Neural networks however is very . In both cases we can however introduce regularization which can help
reduce the model complexity.

\subsection{Logistic regression}
The logistic regression model is a commonly used, and rather simple, way to
perform classification tasks. With logistic regression we assume a binary
response $0$ and $1$ typically and model the probability the follwing way:
$$Pr(Y_i = 1 | \bm{x}_i) = \frac{1}{1 + \exp(\bm{x}_i \bm{\beta})} = \sigma(\bm{x}_i \bm{\beta})$$
, where $\sigma$ is obviously the sigmoid-function as already described. We then
can do classification based on this, where we typically classify to $1$ if this
probability is bigger than $0.5$ (this is the boundary we will use here) and $0$
if lower than $0.5$. In order to estimate the $\beta$ parameters we use the
likelihood such a model, given by \cite[s.~4.4]{hastie2009elements}:
$$l(\bm{\beta}) = \sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta})))$$
, where the estimates $\hat{\bm{\beta}}$ become the ones which maximize this
likelihood. The cost-function then becomes:
$$C(\bm{\beta}) = -\sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta}))).$$
To minimize we find the gradient \cite[s.~4.4]{hastie2009elements}:
$$\frac{\partial C(\bm{\beta})}{\partial \bm{\beta}} = -\sum_{i=1}^{n}\bm{x}_i^T(y_i - Pr(Y_i = 1 | \bm{x}_i)) = -\bm{X}^T(\bm{y} - \bm{p}).$$
We then can use the Newton-Raphson method on this in order to estimate the $\bm{beta}$.

\section{Results}

\subsection{General optimization}

\subsection{The franke-function}

\subsection{Wisconsin Cancer data}

\section{Analysis}
What we can see from the ...

For the franke-function one possible improvement would be to add some different
regularizers, like for example seeing if we get improvement by using $l_1$
regularization instead of $l_2$ regularization. Another thing that could be
interesting to check out is to see how the amount of data here influences the
performance of this neural network.  One would expect that as we increase the
amount of data, that the neural network will eventually perform better than
ridge/lasso. It could be interesting to study what the boundary for this is.

Another possible improvement is that we have only tweaked some of the
hyperparameters of a neural network. Perhaps the most natural parameter to tweak
which we have left out is the momentum parameter, which we just kept fixed at
$0.9$, but this is a parameter which can have some influence over the learning
of the model. The different time-decays we have used has also been somewhat
limited, as we have only used it with stochastic gradient descent, and only in
this case used one quite basic time-decay. In general we have also used just
simple grid-search for hyperparameter-tuning, while one could use more advanced
techniques/libraries.

Lastly a potential problem I have already highlighted is how we have quite few
observations in the cancer data case. This meant our validation data in
particular became smaller than what we really wanted. One could perhaps try a
bigger split. An even better approach would be to simply split into a train-test
and then use cross-validation for selecting the hyperparameters and then
evaluate on the final model. The reason why I opted to not take this approach in
my case is mainly that cross-validation is much more computationally heavy, so
running the programs would take much longer, and they already take quite a bit
of time to run.

\section{Conclusion}

\bibliography{./sources.bib}

\end{document}
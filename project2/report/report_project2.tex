\documentclass{article}
\title{Project 2 \\ FYS-STK3155/4155}
\author{Magnus Bergkvam}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}
\usepackage{listings}

\graphicspath{{../figures/}}


\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
In this project we tested out performances of neural networks for regression and
binary classification purposes, and compared this with the linear models we
explored in the previous project \cite{githubrepoproject1} (in the case of
regression), as well as our own implementation of logistic regression (for
classification). We here implemented our own neural network code from scratch,
basing the training on the backpropagation-algorithm.  With gradient
optimization being a central part of the development of both neural networks and
logistic regression we also dove into various ways of optimizing the cost
function through the Newton-Raphson method, in particular. We also explored what
effect the various hyperparameters introduced in the methods have on how quick we
are able to optimize the cost, and the performance of the final model. In
particular we explored the effect of the learning-rate and
regularization-parameter, and we looked at various methods for optimizing
through the Newton-Raphson method, including ordinary gradient descent,
stochastic gradient descent, adagrad, rmsprop and adam. For the regression we
used the franke-function which we introduced in project 1
\cite{githubrepoproject1}, while for classification we used breast cancer data
from wisconsin \cite{sklearncancerdata} \cite{breastcancerwisconsin}. What we
saw was that for the franke-function with regularization we did not get any
major performance gains compared to the simpler linear models we tested in
project 1 \cite{githubrepoproject1}. For the cancer data we also saw that while
the neural networks did perform a little better than logistic regression,
however the logistic regression also managed to give us very good results.
Throughout the project we have manifested again that just because neural
networks might be very powerful and impressive in certain tasks, there are still
cases where simpler models will outperform them. We also saw which parts of a
neural net was most instrumental for good model performance, with especially the
learning-rate, regularization parameter, the learning-rate scheduler, and the
amount of epochs being the most instrumental for cost-minimization, while
activation functions and hidden layer sizes didn't matter all that much.

\section{Introduction}
In the last $10$ or so years, neural networks have really exploded in
popularity. Various different types of neural networks have proved effective for
various different applications all from speech-recognition, to image processing,
to complex language tasks. But are they always the best option, and how do we
get the most performance out of them?

These are some of the things which we aim to answer here. To do this we will
implement a rather simple multi-layer perceptron model from scratch using the
backpropagation algorithm. We will then use this implementation in a
regression-setting, generating data from the franke-function as we did in
project $1$, and of course compare this to the results we got with the linear
models we explored in project $1$. We will also use our neural network
implementation in a classification setting, where we are trying to classify
whether a patient has cancer or not based on various medical features.
Additionally we will explore various methods of gradient optimizations, as both
the methods we will deal with in this project (that is neural networks and
logistic regression) have cost-functions which one would typically solve using
some sort of gradient method.  As already mentioned we will implement and
explore performances based on plain gradient descent, stochastic gradient
descent, adagrad, rmsprop and adam. For all our different methods we will
compare our own implementations against established libraries, namely tensorflow
and scikit-learn.

There are mainly two reasons why we are specifically focusing on using neural
networks in this project. One is, as already mentioned, that they have become
very popular in the last decade or so, with them being used for more and more
tasks. With their usage increasing it becomes important to understand the way a
model works, and how we get the most amount of performance out of them by
tweaking the various different parts of the network. Additionally since they, in
comparison to a lot of other models, are quite computationally expensive it is
important to realize which of the parameters are the most relevant to tweak, and
which are not so relevant as this can save a lot of time. Another reason is
because of the universal approximation theorem applying to neural networks. We
will go more deeply into this in \ref{univ-approx-thm}, but the essence is that
a sufficiently big neural network should be able approximate any function
multidimensional function to an arbitrary accuracy
\cite[s.~13.5]{lecutenotes13}. We will see that this doesn't necessarily
guarantee us a good model.

\section{Methods}
\subsection{Basic optimization data}
\label{basicoptdesc}
In part of our analysis we will explore the different parameters for the
different gradient methods. For this we will just use some simple arbitrary data
generated from a fourth order polynomial. The polynomial I have chosen to
generate the data from is:
$$f(x) = 4 - 6 x^2 + x^4.$$
We then just generate a grid of values $\bm{x}$ on $[-2, 2]$ and generate the
response as follows:
$$\bm{y} = f(\bm{x}) + \bm{epsilon}$$
, where $\epsilon_i \sim N(0, 0.2^2)$, and $f$ is applied element-wise.  We will
use the linear regression (mse) and lasso regression (mse and $l_2$ norm), and a
design matrix with fourth-order polynomial features. This way analytical
least-squares solution should give us optimal parameters close to $(4, 0, -6, 0,
      1)^T$. We will mainly explore the effect that the learning-rate and
regularization terms have, and also quickly see the effect of the amount
of epochs and minibatch-size can have. We wil also compare for a fixed
amount of epochs, which of the methods perform the best, giving an
indication to which extent the method of optimization is relevant.

\subsection{The franke function}
The franke-function is a $2$-dimensional function which we already have explored
in project $1$ \cite{githubrepoproject1}, so I won't go too deeply into this
here. The function is given by:
\begin{align*}
      f(x,y) & = \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
             & +\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}
Again we will draw 200 of random observations $\bm{x}_1$ and $\bm{x}_2$, with
$(x_1)_i \in [0, 1]$, and $(x_2)_i \in [0, 1]$ for $1 \leq i \leq 200$. The response will be generated by:
$$\bm{y} = f(\bm{x}_1, \bm{x}_2) + \bm{\epsilon}$$
, with $\epsilon_i \sim N(0, 0.1^2)$ (we apply $f$ element-wise). We will try to
fit a neural network to this data with this time only $\bm{x}_1$ and $\bm{x}_2$
as the response. In project $1$ we added polynomial features of various degrees
to fit to a model. Here we do not need to do this as we can just add enough
nodes and the universal approximation theorem assures us that the network should
do a good job of approximating the function.

\subsection{The wisconsin cancer data}
The wisconsin cancer data \cite{breastcancerwisconsin} is a dataset containing
breast cancer cases, with a response-variable indicating wether the patient has
breast-cancer or not, with some corresponding medical features. This is clearly
a binary classification case, which makes this suitable for both neural networks
and logistic regression models. We will use the dataset included in
\textit{sklearn.datasets} \cite{sklearncancerdata}. Loading it into python and
splitting into train, validation and test we do by:

\begin{lstlisting}
X, y = load_breast_cancer(return_X_y=True)
y = y.reshape(len(y), 1)

# Train-test-validation splitting
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.4)
\end{lstlisting}

We split into train, test and validation sets because we will evaluate a lot of
models, and then it becomes increasingly important to evaluate the final
selected model on some test set in order for us to get a reliable final metric.
The data is limited to $569$ observations with each having $30$ explanatory
variables. This makes both the validation set and test set rather small, so with
around $100$ and $70$ observations in each respectively. We then need to be a
little skeptical towards the final metrics, as the metrics may be a little
specific to the split of data.

\subsection{The Newton-Raphson method}
Very central to the numerical optimization we do is the Newton-Raphson method.
This is a method which allows us to find the minimum of some function by using
its gradient. The method has it's origins in the Taylor-expansion of the
function we wish to minimize. In our case we use this to minimize cost-functions
for the models we use. Let $C(\bm{\beta}; \bm{X}, \bm{y})$ be some general
cost-function depending on some design matrix $\bm{X}$ and a response vector
$\bm{y}$. We wish to find the $\bm{\beta}$ which minimizes this cost, given
$\bm{X}$ and $\bm{y}$, i.e. we minimize $C$ with respect to $\bm{\beta}$, while
$\bm{X}$ and $\bm{y}$ are obviously fixed. We can then approximate the cost using a
Taylor-expansion of second order around some value $\bm{\beta}^*$, in which case
we get:
\begin{align*}
      C(\bm{\beta}, \bm{X}, \bm{y}) & \approx C(\bm{\beta}^*; \bm{X}, \bm{y}) + \frac{\partial C(\bm{\beta}^*; \bm{X}, \bm{y})}{\partial \bm{\beta}}(\bm{\beta} - \bm{\beta}^*) + \frac{1}{2} \frac{\partial^2 C(\bm{\beta}; \bm{X}, \bm{y})}{\partial^2 \bm{\beta}}(\bm{\beta} - \bm{\beta}^*)^2 \\
                                    & = C(\bm{\beta}^*; \bm{X}, \bm{y}) + \nabla_{\bm{\beta}} C(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) + \frac{1}{2} \bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*)^2
\end{align*}
Where $\nabla_{\bm{\beta}} C$ is the gradient of $C$ w.r.t. $\bm{\beta}$ and
$\bm{H}_{\bm{\beta}}$ is the Hesse-matrix (second derivative) of $C$ w.r.t.
$\bm{\beta}$. This should be a good approximation for $C$ sufficiently close to
$\bm{\beta}^*$. We then can find the derivative of this w.r.t. $\bm{\beta}$:
\begin{align*}
      \frac{\partial C(\bm{\beta}, \bm{X}, \bm{y})}{\partial \bm{\beta}} & \approx
      \nabla_{\bm{\beta}} C(\bm{\beta}^*) + \bm{H}_{\bm{\beta}}(\bm{\beta}^*)(\bm{\beta} - \bm{\beta}^*) \\
\end{align*}
If we set this to $0$ we further get:
$$\bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) = -\nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
which leads to
$$\bm{\beta} - \bm{\beta}^* = - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
and finally
$$\bm{\beta} = \bm{\beta}^* - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$

This tells us that the minimum of the Taylor-expansion of the cost-function,
using any $\bm{\beta}^*$ is given as above. The problem with this is that this
is using the Taylor-expansion, which is only reliable close to $\bm{\beta}^*$ so
chances are that using this formula will not give us a global minimum. However
it generally tends to bring us closer to the minimum we are looking after. What
we then usually do is to start with some initial guess for the optimal
$\bm{\beta}$, which we denote $\bm{\beta}^{(0)}$. We then for $k=1, \dots$ let
$\bm{\beta}^{(k)} = \bm{\beta}^{(k-1)} -
      \bm{H}_{\bm{\beta}}(\bm{\beta}^{(k-1)})^{-1} \nabla_{\bm{\beta}}
      C(\bm{\beta}^{(k-1)})$ all the way until we have convergence.

\subsection{Learning-rate schedulers}
Something which often is problematic when using the Newton-Raphson method as
described over is that we need to estimate the Hessian matrix and also invert
it. First of all finding a way to calculate the second derivative can be a
difficulty in and of itself, but it may also be computationally expensive to do
so. Not only that, we also need to invert this matrix which can also become very
computationally expensive, and perhaps also numerically unstable with higher
dimensional hesse matrices (remember that the Hessian matrix is a square matrix
with the same dimensions as $\bm{\beta}$ in both width and height).  Inverting
this matrix then may not be a problem if we only have a few parameters, but for
example when dealing with logistic regression with lots of features, or neural
networks with lots of hidden nodes and layers this becomes unviable. We
therefore will use various methods to approximate this hessian matrix.

\subsubsection{Ordinary gradient descent}
Ordinary gradient descent is a very simple way of approximating the inverse of
the Hessian matrix. Here we simply approximate it with some numerical
learning-rate, which we denote $\eta$. This way our algorithm becomes:
$$\bm{\beta}^{(k)} = \bm{\beta}^{(k-1)} - \eta \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)}).$$
Often we may just keep this fixed, but we can also make it depend on $k$. For
example one may wish to start with a higher learning-rate and make this decrease
with the number of iterations. This method I will refer to as using time-decay.
When using gradient descent for training our model $\eta$ then becomes a
hyperparameter in our model. Often this hyperparameter is rather important in
determining the fit of the model. We often want a rather high learning-rate as
this will lead to faster convergence towards some optimum, however if we make it
to big we may not get convergence at all. I will mainly throughout the code
simply set up a grid of learning-rates and fit to all of them, and simply choose
the learning-rate which gives the best performance, but there are countless
other more advanced way of tuning this parameter.

\subsubsection{Stochastic gradient descent}
Stochastic gradient descent is very similar to ordinary gradient descent, except
the fact that instead of training for each epoch on the whole dataset we split
the data into $n$ randomly chosen mini-batches each of some specified size (this
is what \textit{sgd\_size} in our code stands for). Then we do training over all
of the $n$ mini-batches for each epoch, while the rest of the method is the same.

\subsection{Momentum}
One problem with gradient descent and stochastic gradient descent which one
often can see is that it can be quite slow to learn. Ideally we would like the
model to minimize the cost/loss as quickly as possible, and therefore one might
want to use methods designed to accelerate this learning process. One relatively
simple such method is using a momentum, which often accelerates learning quite
drastically especially in cases with high curvatures and small consistent or
noisy gradients \cite[s.~8.3.2]{goodfellow2016deep}. This method introduces a
variable $\bm{v}$ which describes the direction and speed which the parameter
$\bm{\theta}$ is moving in parameter space \cite[s.~8.3.2]{goodfellow2016deep}, much
like the role of velocity in physics. To calculate this $\bm{v}$ we need a
hyperparameter $\alpha \in [0, 1)$ and we initialize $\bm{v} = \bm{0}$. Then for
each learning-iteration we update $\bm{v}$ the following way:
$$\bm{v} = \alpha \bm{v}_{prev} - \eta \Delta_{\bm{\theta}} C(\bm{x}, \bm{y}, \bm{\theta})$$
We then can simply update the $\bm{\theta}$ as follows:
$$\bm{\theta} = \bm{\theta}_{prev} + \bm{v}$$
We then see that the value of the $\bm{v}$ will depend on the current cost-gradient
w.r.t. $\bm{\theta}$ as well as the previous $\bm{v}$, which again depends on
the previous cost-gradient w.r.t. $\bm{\theta}$ and the previous previous
$\bm{v}$ and so on. Therefore we see that $\bm{v}$ is influenced by all the
previous cost-gradients, weighted differently, in contrast to just using the
current cost-gradient, and granted they point in a somewhat similar direction,
the $\bm{v}$ will grow bigger and bigger in size. It is therefore not that
difficult to see that this can give quicker learning.


Keep in mind that in theory there is nothing stopping us from combining momentum
with any of the other methods we discuss here, however methods like adam already
incorporates a form of momentum \cite[s.~8.5.3]{goodfellow2016deep}, so we might
not get much quicker learning by using it, if any at all.

\subsubsection{AdaGrad}
\label{adagradsec}
AdaGrad is another way designed to give faster learning. This is an algorithm
which uses adaptive learning-rates. These have a difference in that each axis of
the parameter-space $\bm{\theta}$ essentially get a separate learning-rate
\cite[s.~8.5]{goodfellow2016deep}, while these separate learning-rates change/adapt
throughout the learning process, which is a different approach to using momentum.
AdaGrad is a rather simple way of achieving a good such adaptive learning-rate.
It scales down the parameters with with big updates, and scales up the ones with
small corresponding updates up. For AdaGrad we introduce one variable $\bm{r}$
which we initially set to $\bm{0}$, as well as a small constant $\delta$ which
we typically set to $10^{-7}$. The algorithms is as follows until the
stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\bm{r}_{prev} + \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta
                  \frac{1}{\delta + \sqrt{\bm{r}}} \odot \bm{g}$ and update
            $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).

\end{itemize}

\subsubsection{RMSProp}
RMSProp is much like AdaGrad, but is designed to be more robust for
cost-functions with very complex structures like neural nets. Unlike AdaGrad
where we shrink the learning-rates using the sizes of all the previous partial
derivatives of the cost-function for all the axis RMSProp focuses less on the
history of gradients from very long ago, but otherwise using much of the same
approach. The RMSProp algorithm requires therefore a decay-rate $\rho$ as a
hyperparameter as well, along the $\bm{r}$ which we again initialize to be
$\bm{0}$, as well as a small constant $\delta$, which typically is set to
$10^{-6}$. Then we do the following until the stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\rho \bm{r}_{prev} + (1 - \rho) \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta \frac{1}{\sqrt{\delta + \bm{r}}}
                  \odot \bm{g}$ and update $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).
\end{itemize}

\subsubsection{Adam}
Adam is a very popular learning-rate scheduler, which is probably the method we
will tackle which most often gives the best results. Our implementation is
essentially as described in algorithm 8.7 in \textit{Deep Learning}
\cite[s.~8.6.1]{goodfellow2016deep}. We start the algorithm by initializing the
parameter $\bm{\theta}$ which we would like to optimize the cost/loss w.r.t.
(typically we initialize this by drawing it randomly from for example the normal
distribution which is what we have done in this project). We also set beforehand
$\bm{s} = \bm{0}$, $\bm{r} = \bm{0}$ and $t=0$. Here the algorithms is as
follows until we meet the stopping criterion:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Increment $t$ by $1$ (i.e. set $t=t_{prev}+1$).
      \item Update $\bm{s}$ to become $\rho_1 \bm{s}_{prev} + (1 - \rho_1)\bm{g}$.
      \item Update $\bm{r}$ to become $\rho_2 \bm{r}_{prev} + (1 - \rho_2)g \odot
                  g$ ($\odot$ again being the Hadamard-product).
      \item Correct the bias in the first moment by: $\hat{\bm{s}} = \frac{\bm{s}}{1 - \rho_1^t}$.
      \item Correct the bias in the second moment by: $\hat{\bm{r}} = \frac{\bm{r}}{1 - \rho_2^t}$.
      \item Finally calculate $\Delta \bm{\theta} = -\eta
                  \frac{\hat{\bm{s}}}{\sqrt{\hat{\bm{r}}} + \delta}$ and update $\bm{\theta}$
            accordingly (i.e. set $\bm{\theta} = \bm{\theta}_{prev} + \Delta
                  \bm{\theta}$).
\end{itemize}
Where $\eta$ is the step-size (or learning-rate as we will call it), $\rho_1$
and $\rho_2$ are the decay-rate which must be in $[0, 1)$ (typically we set the
defaults to $\rho_1 = 0.9$ and $\rho_2 = 0.999$), $\delta$ is just a small
constant (we set it to $10^{-8}$). Note that we can in theory drop the minibatch
sampling and still get a functioning algorithm, but this will often give much
worse results.

\subsection{Automatic differentiation}
What is clear is that in our optimization it will be central to calculate the
gradient of some loss function with respect to some parameter, however it is not
always easy to find analytical gradients by hand. Automatic differentiation is
an algorithmic approach to calculating the gradients of functions. We will take
some usage of this in our code through the python-library jax
\cite{githubrepojax}, but this is a somewhat minor part of the project so I
won't go too much into detail about this. This utilizes clever applications of
the chain rule in order to calculate the gradients of functions without
compromising on accuracy (it usually gives almost exactly the same results as an
analytical expression).  We will utilize this in some of our optimizations
instead of calculating gradients by hand, and for adding support in our code so
that users don't have to manually calculate gradients by hand, which can be
time-consuming, and a potential for errors in the calculations.

\subsection{Neural networks}
There are many different implementations of neural networks out there. We will
in this project focus on one of the simplest ones to implement, namely the
multi-layer-perceptron. The multi-layer perceptron consists of a input layer
with $p$ nodes (granted we have $p$ covariates), as many hidden layers as we
want, each of which can have also as many nodes as we want, and then an output
layer of the same dimensionality as our response. For our notation in this
section we will assume we have $L$ total layers (excluding the input layer), or
equivalently $L-1$ hidden layers. Linking the layers together is a set of
weights, where each node in the layer before is connected to each node in the
next layer. Additionally each layer has its own weight matrix $W^{(l)}$ and bias
vector $b^{(l)}$ for $1 \leq l \leq L$. Additionally each layer has its own
activation function, which we denote $f^{(l)}$, again for $1 \leq l \leq
      L$. Typical choices of activation functions are sigmoid, relu or leaky relu,
which are the ones we will explore in this project. For the final layer we
instead call this the output function. The output function is typically tailored
to whichever data we have. If we for example want to do regression we often just
set this to the identity-function, i.e. we do not transform the activations at
all. In the case where we are doing classification we this choice is no longer a
good one as the identity function is not bounded, and we in that case want the
output to be a probability between $0$ and $1$, so then a sigmoid-function for
example is a good choice. Predictions are made using forward propagation which
we forward through the network (this algorithm we describe in
\ref{forwardprop}), calculating the next layer activations based on the layer
activation in the layer before, until we are at the final layer. Here we will
follow the notation of \textit{Deep Learning}\cite{goodfellow2016deep} by
denoting $\bm{a}^{(l)}$ as the activations in layer $l$ before applying the
activation functions and $\bm{h}^{(l)}$ as the final activations in layer $l$
(i.e. after applying the activation functions). Keep in mind that the final
predictions then will be $\bm{h}^{(L)}$, while the activations in the inputs
(which is just the values of the features) are denoted $\bm{h}^{(0)}$. For
fitting the networks we use the backpropagation algorithm, where we for some
given some design matrix $\bm{X}$ we fit $\hat{\bm{y}} = \bm{h}^{(L)}$ to match
some target output $\bm{y}$ for a given cost-function $C(\hat{\bm{y}}, \bm{y})$
as good as possible.

\subsubsection{Regularization}
We will also implement basic support for regularization. This requires a
regularization function $\Omega(\bm{\theta})$, where $\bm{\theta}$ in this case
corresponds to all the parameters. Additionally we add a regularization parameter
$\lambda$ which scales the extent of regularization we apply. Here we will only
add $l_2$ regularization, so $\Omega(\bm{\theta}) = \frac{1}{2}\bm{\theta}^T
      \bm{\theta}$. How the regularization is used we will go into mainly in
\ref{backprop}, but we will also need the gradient of this regularizer with
respect to the weights in each layer. For the $l_2$ regularization it is easy to
see that this becomes:
$$\Omega(\bm{\theta}) = \frac{1}{2} \left( \sum_{l = 1}^{L} \sum_{i = 1}^{n_{i-1}} \sum_{j = 1}^{n_i} {\bm{W}_{i j}^{(l)}}^2 + \sum_{l = 1}^{L} {\bm{b}^{(l)}}^T \bm{b} \right)$$.
Taking gathering the derivative with respect to $W_{i j}^{(l)}$ we get:
\begin{align*}
      \frac{\partial \Omega(\bm{\theta})}{\partial \bm{W}_{i j}}^{(l)} & = \frac{1}{2} \frac{\partial {\bm{W}_{i j}^{(l)}}^2}{\partial \bm{W}_{i j}^{(l)}} \\
                                                                       & = \bm{W}_{i j}^{(l)}
\end{align*}
Hence we get that $\nabla_{\bm{W}^{(l)}} \Omega(\bm{\theta}) = \bm{W}^{(l)}$.\\
Keep in mind that we do not include regularization on biases.  This is because
we usually are ok with having sizeable values for the biases, more so that we
are with having big values for weights, which often results in high degree of
overfitting. Seeing as there are a lot more weights than biases, more model
complexity is also reduced by penalizing the weights than the biases.

\subsubsection{Forward propagation algorithm}
\label{forwardprop}
With the description of the model done the forward propagation is actually
rather simple. We assume we have $L-1$ hidden layers, with index $L$ being the
output layer. We let us inspire by algorithm 6.3 in \textit{Deep Learning}
\cite[p.~212]{goodfellow2016deep}, with some practical improvements for the code:
\begin{itemize}
      \item Set $\bm{h}^{(0)} = \bm{X}$
      \item \textbf{for} $l = 1, \dots, L$ \textbf{do}
      \item $\bm{a}^{(l)} = \bm{h}^{(l-1)} \bm{W}^{(l)} + \bm{b}^{(l)}$ (here $\bm{b}^{(l)}$ is added for every row).
      \item $\bm{h}^{(l)} = f^{(l)}(\bm{a}^{(l)})$ (here we apply $f$ element-wise)
      \item \textbf{end for}
      \item $\hat{\bm{y}} = \bm{h}^{(l)}$
\end{itemize}
Keep in mind that here that $\bm{X}$, $\bm{y}$, $\bm{a}$-s, $\bm{h}$-s are
allowed to be matrices. Here if $\bm{X}$ is a $n \times p$ matrix, it is easy to
see that $\bm{a}^{(l)}$ and $\bm{h}^{(l)}$ are going to be $n \times n_l$
matrices, i.e. we have that each row represents the activations in layer $l$ for
each observation. Then as a consequence since $\hat{\bm{y}} = \bm{h}^{(L)}$, we
have that each row in the prediction matrix corresponds perfectly to the
prediction of each row of the design/feature matrix.

\subsubsection{Backward propagation algorithm}
\label{backprop}
The backward propagation algorithm is a little more complicated. Again we take
big inspiration from \textit{Deep Learning}, this time algorithm 6.4, with some
tweaks. The backwards propagation algorithm then goes as follows:
\begin{itemize}
      \item Set $\bm{g} = \nabla_{\hat{\bm{y}}} C(\hat{\bm{y}}, \bm{y})$
      \item \textbf{for} $l = L, \dots, 1$ \textbf{do}
      \item Update $\bm{g}$ by setting $\bm{g} = \bm{g} \odot f^{(l)} \prime (\bm{a}^{(l)})$
      \item Calculate $\nabla_{\bm{b}^{(l)}} L(\hat{\bm{y}}, \bm{y}) = \sum_{i=1}^n \bm{g}_{i *}$ (sum over all the rows)
      \item Calculate $\nabla_{\bm{W}^{(l)}} L(\hat{\bm{y}}, \bm{y}) = {\bm{h}^{(l)}}^T \bm{g} + \lambda \nabla_{\bm{W}^{(l)}} \Omega(\bm{\theta})$
      \item \textbf{end for}
\end{itemize}

\subsubsection{Weight and bias initialization}
We see that at the core of neural networks are these weigh-matrices and bias
vectors, and seeing as we want to use an iterative method (namely the
Newton-Raphson method) it becomes clear that we need to initialize the weights
and biases to something. If we were to initialize all the weights/biases to be
$0$ (or some other constant), all the weights/biases would have the same effect
on the loss, and hence the backward-propagation algorithm. Our solution to this
is to draw the weights randomly from the standard-normal distribution, this way
we will be able to break the unwanted symmetry in our learning-process. For
biases one don't always bother initializing these randomly (although there is
nothing hindering us to do so). What we will do initialize all the biases to the
same value in every layer (we will set then to $0.1$). Setting it to a small
positive value will often activate the neurons when we are using a
relu-function (see \ref{relu-desc}), which again hinders the derivative of the
relu to be $0$, and this enables the network to learn
\cite[s.~6.3.1]{goodfellow2016deep}.

\subsubsection{The universal approximation theorem}
\label{univ-approx-thm}
The universal approximation theorem tells us that if we have a neural network
with one hidden layer and a non-linear activation-function in the hidden layer,
if we have enough nodes in the hidden layer we should be able to approximate any
continous multi-dimensional function to any accuracy
\cite[s.~13.5]{lecutenotes13}. Keep in mind that this is merely to the
training-data. A neural network can be able to approximate the training-data
very well, without necessarily getting that great performance for new data.

\subsection{Activation functions}
For this project we will explore three different activation functions for the
hidden layers.
\subsubsection{Sigmoid}
The sigmoid activation function limits the activations of the neurons between
$0$ and $1$. The sigmoid activation function is given as the following:
$$\sigma(x) = \frac{1}{1 + \exp(-x)}$$
With the derivative being (this is quite straightforward to calculate):
$$\sigma'(x) = \sigma(x) (1 - \sigma(x))$$
Additionally as mentioned this will also be used as an output-function when we
are doing binary classification, due to the fact that the final neuron
activations will be between $0$ and $1$.

\subsubsection{Relu}
\label{relu-desc}
Another popular activation function we will use is the relu activation function.
Unlike the sigmoid function this does not have a upper bound. Essentially the
idea is to. The function then can be expressed by:
$$\text{relu}(x) = \max(x, 0)$$
The derivative then becomes:
$$\text{relu}'(x) = \begin{cases}
            0 \qquad & \text{if } x < 0 \\
            1        & \text{if } x > 0
      \end{cases}$$
Keep in mind that the derivative is not defined for $x=0$. This however is
usually unproblematic as we almost never get activations being exactly $0$. In
our implementation we will just set the derivative to $0$ if the activation is
exactly $0$ (but again this should more or less never happen).

\subsubsection{Leaky relu}
The leaky relu is a generalization of the relu-function which unlike the
relu-function doesn't completely neutralize negative values.  Technically this
makes this function not bounded below, however with $\delta$ being rather small
severely limits the size of the negative activations, while no such shrinkage is
applied for the positive activations of course. The leaky relu is given by:
$$\text{lrelu}(x) = \begin{cases}
            x \qquad & \text{if } x > 0    \\
            \delta x & \text{if } x \leq 0
      \end{cases}$$
The derivative then becomes:
$$\text{lrelu}'(x) = \begin{cases}
            \delta \qquad & \text{if } x < 0 \\
            1             & \text{if } x > 0
      \end{cases}$$
We see one major difference between this and the relu is that for negative
activations, the derivative of this will not be $0$, but $\delta$. This means
that the network will be able to learn on activations which are negative as
well, albeit likely slower than the positive cases.

\subsection{Cost functions}
In this project we will use mainly two activation functions, namely mse/sse for
regression and cross-entropy for binary-classification. We will now quickly go
into these.

\subsubsection{MSE/SSE}
A natural choice of cost-function when dealing with regression tasks is the MSE.
When the target dimensions are always $1$ (which they are in this project), the
MSE is defined like:
$$\text{mse}(\bm{h}^{(L)}, \bm{t}) = \frac{1}{n} \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = \frac{1}{n}(\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
The derivative with respect to $\bm{h}^{(L)}$ when becomes:
$$\frac{\partial \text{mse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = \frac{2}{n} (\bm{h}^{(L)} - \bm{t})$$
We will also use the total sum of squares (SSE). Minimizing this is the same as
minimizing the MSE, but with one less floating-point operation. The SSE is given
by:
$$\text{sse}(\bm{h}^{(L)}, \bm{t}) = \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = (\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
and its derivative then becomes:
$$\frac{\partial \text{sse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = 2(\bm{h}^{(L)} - \bm{t})$$
The only potential problem with using SSE instead of MSE is that the gradient
will then be dependent on the amount of observations we are using, which can be
problematic with stochastic gradient descent. However in this project we will
set a fixed size of each minibatch, which guarantees that each training-set sent
into this cost/loss function will be of the same size, so this becomes
unproblematic.

\subsubsection{Cross-entropy}
For binary classification we will use cross-entropy as out cost-function.
Minimizing the cross-entropy is equivalent to maximizing the likelihood. We have
the cross-validation given by:
$$\text{cross-entropy}(\bm{h}^{(L)}, \bm{t}) = -\sum_{i=1}^{n} ( t_{i} \log(h^{(L)}_i) + (1 - y) \log(1 - h^{(L)}_i) )$$
With the gradient w.r.t. $h^{(L)}$ then becoming:
$$\frac{\partial \text{cross-entropy}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = (\bm{h}^{(L)} - \bm{t}) / ((1 - \bm{h}^{(L)}) \odot \bm{h}^{(L)})$$
Here the division is element-wise. One thing to note is that we see that if one of
our predictions $h^{(L)}_i$ is very close to $0$ while the target value is $1$
or vice versa, which especially can happen in the first iterations, perhaps the
most with some sort of unbounded activation function in the last layer, the
gradient can become very big. If we get too big of a gradient this can be
problematic for many reasons, with one being we can get values too big for the
computer to calculate. In our code we will solve this by clipping the
predictions in the cross-entropy-gradient, so that they don't become too close
to $0$ or $1$ so that this doesn't become a problem. Additionally we will add a
small $\delta$ inside the log of the cross-entropy for the same reason.

\subsubsection{Notation in the code}
Keep in mind that in the code the cost-functions often takes $\hat{y}$ and $y$
instead of $\bm{h}^{(L)}$ and $\bm{t}$ respectively. This essentially means the
same thing, but makes more sense with the notation for usage outside of neural
networks.

\subsection{Accuracy score}
For classification methods, after we have fitted the models and made predictions
we may want to evaluate a metric of how good our model really is. One intuitive
such metric we will use a lot is the accuracy score. The accuracy-score
essentially is just the proportions of datapoints we have classified correctly
on some data. The accuracy score can then be defined.
Using numpy this can simply be written as:
\begin{lstlisting}
np.mean(y_pred == y)
\end{lstlisting}
, for some predictions \textit{y\_pred} and some target \textit{y}. Keep in mind
that while this can tell us the overall successes/failures of classifications it
does not tell us what kinds of prediction errors we have made. For example in a
binary-classification problem we may be interested in finding the amount of
predictions we have made to be $1$ while the target was $0$, or vice versa, as
one of these predictions errors may be more severe than the other. For these
cases we have a confusion-matrix plotting the predictions against the target in
a heatmap. We will use some such plots in this project as well.

\subsection{Model complexities}
One final thing to consider which will be very important for our results is the
model-complexity of neural networks and logistic regression. Logistic regression
requires just as many parameters as we have explanatory variables, so the model
complexity is rather low, at least in the case where we don't have too many
covariates. Neural networks however is very . In both cases we can however
introduce regularization which can help reduce the model complexity.

\subsection{Logistic regression}
The logistic regression model is a commonly used, and rather simple, way to
perform classification tasks. With logistic regression we assume a binary
response $0$ and $1$ typically and model the probability the follwing way:
$$Pr(Y_i = 1 | \bm{x}_i) = \frac{1}{1 + \exp(\bm{x}_i \bm{\beta})} = \sigma(\bm{x}_i \bm{\beta})$$
, where $\sigma$ is obviously the sigmoid-function as already described. We then
can do classification based on this, where we typically classify to $1$ if this
probability is bigger than $0.5$ (this is the boundary we will use here) and $0$
if lower than $0.5$. In order to estimate the $\beta$ parameters we use the
likelihood such a model, given by \cite[s.~4.4]{hastie2009elements}:
$$l(\bm{\beta}) = \sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta})))$$
, where the estimates $\hat{\bm{\beta}}$ become the ones which maximize this
likelihood. The cost-function then becomes:
$$C(\bm{\beta}) = -\sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta}))).$$
To minimize we find the gradient \cite[s.~4.4]{hastie2009elements}:
$$\frac{\partial C(\bm{\beta})}{\partial \bm{\beta}} = -\sum_{i=1}^{n}\bm{x}_i^T(y_i - Pr(Y_i = 1 | \bm{x}_i)) = -\bm{X}^T(\bm{y} - \bm{p}).$$
We then can use the Newton-Raphson method on this in order to estimate the $\bm{beta}$.

\section{Results}

\subsection{General optimization}
The \textbf{optimization\_basic.py} in the project repo
\cite{githubrepoproject2code} performs various experimentation on some generated
data as described in \ref{basicoptdesc}, and explores the various effects the
various parameters have when using gradient methods. For the amount of epochs we
did a quick test by four-doubling the amount of epochs. Table
\ref{epochs-varying} shows the results of optimizing using GD and SGD with
different amount of epochs. What we clearly see is that in general the more
amount of epochs we allow the smaller the cost will be (granted we have
convergence for the learning-rate). This is intuitive as for each iteration we
should get closer to the minimum of the cost, so doing more iterations must also
naturally bring us closer to the minimum.

\begin{table}
      \centering
      \begin{tabular}{| c | c | c |}
            Epochs   & 100  & 400  \\
            COST GD  & 4.51 & 1.56 \\
            COST SGD & 0.13 & 0.04
      \end{tabular}
      \caption{The cost (mean-squared-error) of the model for plain gradient
            descent and stochastic gradient descent, trained with $100$ epochs and
            $400$ epochs. In both cases the model trained on more epochs is
            substantially smaller.}
      \label{epochs-varying}
\end{table}

For stochastic gradient descent we may also be interested in seeing if the size
of the minibatches has an effect on the speed we are able to learn. Table
\ref{minibatchsize-varying} shows the effect at which the minibatch-size
influences how quick we are able to minimize the cost. Here we see that the
lower minibatch-sizes seem to give a little quicker learning, but that  this
effect is not that enormous (perhaps with the exception of the difference
between the minibatch-sizes of $8$ and $32$, where there is quite a difference).

\begin{table}
      \centering
      \begin{tabular}{| c | c | c | c | c |}
            Minibatch-size  & 8      & 12     & 16     & 32     \\
            COST SGD (best) & 0.0421 & 0.0612 & 0.1315 & 0.7856
      \end{tabular}
      \caption{The cost (mean-squared-error) of the model for stochastic
            gradient descent trained on a grad of learning-rates for each
            minibatch-size with the learning-rate giving the lowest cost is chosen. We
            see that in this case the lower minibatch-sizes seem give faster
            learning. Here we have trained using $100$ epochs.}
      \label{minibatchsize-varying}
\end{table}

We also trained the various methods using stochastic gradient descent on some
learning-rate grid. The costs of these functions plotted against the
learning-rates at which the cost was obtained is contained in figure
\ref{learningrate-cost-plot}. We see that generally that the bigger the
learning-rate is the better it is, however we also see that if it gets to high
then we get that the cost diverges towards infinity. We also can see that in all
the cases the learning-rate is very important, with higher learning-rates giving
much quicker learning.

Just from this plot we see quite a big difference between how quick the
different optimization-methods work. Table \ref{gradmethods-comparison} shows
the costs for the different methods of calculating the learning-rates. We see
that in all cases adding momentum increases the speed of learning by quite a
bit (perhaps less so with AdaGrad trained on stochastic minibatches, where this
increase is quite marginal). We see that which method we use does impact the
speed of learning by quite a lot, with the lowest cost (stochastic AdaGrad with
momentum) having more than $100$ times smaller than the highest cost (GD without
momentum). Overall we can see that AdaGrad did seem to perform the best out of
the methods we tried out.

We also check out the performance of each method under a different loss, which
is the ridge-loss. For the previous cases we have explored how the performance
depends some hyperparameters isolated. In ridge regression we introduce another
hyperparameter, namely the regularization parameter $\lambda$. However we may
also want to see how the performance. Since we are dealing with ridge regression
here it makes sense to explore the mse on some test-data, which is what we have
done in the program. This now doesn't only depend on the learning-rate $\nu$
anymore as previously, but also the regularization parameter $\lambda$ (we keep
the rest of the parameters fixed). Figure \ref{heatmapplot-lr-lambda-gd-sgd}
shows a heatmap which tells us how this depends on both the $\nu$ and $\lambda$
for GD and SGD.  What we see is that the best performance, but that a wrong
$\nu$ or $\lambda$ alone can very much ruin the performance. Similarly
\ref{heatmapplot-lr-lambda-adagrad} shows a corresponding plot for AdaGrad. Here
we see that this is much more robust to higher learning-rates than the other
methods, where it seems to take much more for this to diverge. This is probably
related to the fact that AdaGrad vastly downscales the learning in directions
with high historical updates (see \ref{adagradsec}) so this is able to combat
the higher learning-rates.

For both the figure \ref{heatmapplot-lr-lambda-gd-sgd} and
\ref{heatmapplot-lr-lambda-adagrad}, focusing on the regularization parameter
$\lambda$ we see much of the same as we did with normal ridge, where a too low
$\lambda$ did not give that good results because of too little regularization,
while a too high $\lambda$ also caused subpar results due to too much
regularization. However, we also see that we now have another dimension on top
determining the performance, namely that the learning-rate must also be optimal
for the model to be any good. Picking a good model then requires us to do some
kind of hyperparameter-tuning (like a grid-search for example) on both $\nu$ and
$\lambda$.

\begin{figure}
      \centering
      \includegraphics[scale=0.8]{optimizers_plot}
      \caption{The costs trained on specific learning-rates. We see a general
            downward-moving trend, until we get a learning-rate higher than
            $\sqrt{10}$ for the SGD cases, and $10^{-2}$, and
            $10^{-\frac{3}{2}}$ for the GD cases without and with momentum
            respectively, in which case after this we get a diverging cost.}
      \label{learningrate-cost-plot}
\end{figure}

\begin{table}
      \centering
      \begin{tabular}{| c | c | c | c | c |}
            Method name          & GD     & SGD    & AdaGrad (non-sgd) & AdaGrad (sgd) \\
            COST (no momentum)   & 4.5150 & 4.1092 & 1.4655            & 0.0418        \\
            COST (with momentum) & 0.0384 & 0.0488 & 0.0400            & 0.0385
      \end{tabular}
      \caption{Various different learning-rate scheduling methods, with the
            methods being trained for a grid of learning-rates $\nu$, with the best
            cost being selected. Here for the stochastic gradient descent, we
            have used a timedecay for the learning-rate (plain gradient descent
            only). AdaGrad (sgd) here refers to using AdaGrad and training on
            stochastic mini-batches.}
      \label{gradmethods-comparison}
\end{table}

\begin{figure}
      \centering
      \includegraphics[scale=0.8]{optimizers_ridge_gd_sgd}
      \caption{A heatmap of the test MSEs for various learning-rates and
            regularization paramters, using GD/SGD with and without momentum. Here we
            see that the lowest test MSEs for all the methods are somewhere in the
            middle of each heatmap. We also see that too low or high of a
            learning-rate $\nu$ gives very poor or diverging costs, while too high or
            low regularization parameter $\lambda$ also gives worse performance.}
      \label{heatmapplot-lr-lambda-gd-sgd}
\end{figure}

\begin{figure}
      \centering
      \includegraphics[scale=0.8]{optimizers_ridge_adagrad}
      \caption{A similar heatmap to \ref{heatmapplot-lr-lambda-gd-sgd}. We see
            much of the same patterns here, with one difference being that apart
            from the non-stochastic model without momentum, we are able to have
            much higher learning-rates without the costs diverging.}
      \label{heatmapplot-lr-lambda-adagrad}
\end{figure}


\subsection{The franke-function}

\subsection{Wisconsin Cancer data}

\section{Analysis}
What we can see from the ...

For the franke-function one possible improvement would be to add some different
regularizers, like for example seeing if we get improvement by using $l_1$
regularization instead of $l_2$ regularization. Another thing that could be
interesting to check out is to see how the amount of data here influences the
performance of this neural network.  One would expect that as we increase the
amount of data, that the neural network will eventually perform better than
ridge/lasso. It could be interesting to study what the boundary for this is.

Another possible improvement is that we have only tweaked some of the
hyperparameters of a neural network. Perhaps the most natural parameter to tweak
which we have left out is the momentum parameter, which we just kept fixed at
$0.9$, but this is a parameter which can have some influence over the learning
of the model. The different time-decays we have used has also been somewhat
limited, as we have only used it with stochastic gradient descent, and only in
this case used one quite basic time-decay. In general we have also used just
simple grid-search for hyperparameter-tuning, while one could use more advanced
techniques/libraries.

Lastly a potential problem I have already highlighted is how we have quite few
observations in the cancer data case. This meant our validation data in
particular became smaller than what we really wanted. One could perhaps try a
bigger split. An even better approach would be to simply split into a train-test
and then use cross-validation for selecting the hyperparameters and then
evaluate on the final model. The reason why I opted to not take this approach in
my case is mainly that cross-validation is much more computationally heavy, so
running the programs would take much longer, and they already take quite a bit
of time to run.

\section{Conclusion}

\bibliography{./sources.bib}

\end{document}
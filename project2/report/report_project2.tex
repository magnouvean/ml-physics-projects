\documentclass{article}
\title{Project 2 \\ FYS-STK3155/4155}
\author{Magnus Bergkvam}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}


\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
In this project we tested out performances of neural networks for regression and
binary classification purposes, and compared this with the models we explored in
the previous project \cite{githubrepoproject1}, as well as our own
implementation of logistic regression. We here implemented our own neural
network code from scratch, basing the training on the backpropagation-algorithm.
We also tested out various different optimization techniques for optimizing the
loss, including stochastic gradient descent and the usage of different
schedulers, like adam, rmsprop and adagrad. We tested out regression on the
franke-function which we also studied in project 1 \cite{githubrepoproject1},
while for classification we used breast cancer data from wisconsin
\cite{sklearncancerdata} \cite{breastcancerwisconsin}. What we saw was that for
the franke-function with regularization we did not get any major performance
gains compared to the simpler linear models we tested in project 1
\cite{githubrepoproject1}. For the cancer data we also saw that while the neural
networks did perform well, so did logistic regression with some regularization,
so there was little to no gain here either. Throughout the project we have
manifested again that just because neural networks might be powerful and
impressive in and of themselves, they are not necessarily the best anyhow. We
also saw which parts of a neural net was most instrumental for good model
performance, with especially the learning-rate and regularization parameter
being the most instrumental, while activation functions and hidden layer sizes
didn't matter all that much.

\section{Introduction}
In the last $10$ or so years, neural networks have really exploded in
popularity (find source). They have proved to be very good for various tasks
like image and speech recognition, natural language processing and other complex
tasks. Often

\section{Methods}
\subsection{The Newton-Raphson method}
Very central to the numerical optimization we do is the Newton-Raphson method.
This is a method which allows us to find the minimum of some function by using
the gradient its gradient. The method has it's origins in the Taylor-expansion
of the function we wish to minimize. In our case we use this to minimize
cost-functions for the models we use. Let $C(\bm{\beta}; \bm{X}, \bm{y})$ be
some general cost-function depending on some design matrix $\bm{X}$ and a
response vector $\bm{y}$. We wish to find the $\bm{\beta}$ which minimizes this
cost, given $\bm{X}$ and $\bm{y}$, i.e. we minimize $C$ with respect to
$\bm{\beta}$, while keeping $\bm{X}$ and $\bm{y}$ fixed. We can then approximate
the cost using a Taylor-expansion of second order around some value
$\bm{\beta}^*$, in which case we get:
\begin{align*}
    C(\bm{\beta}, \bm{X}, \bm{y}) & \approx C(\bm{\beta}^*; \bm{X}, \bm{y}) + \frac{\partial C(\bm{\beta}^*; \bm{X}, \bm{y})}{\partial \beta}(\beta - \bm{\beta}^*) + \frac{1}{2} \frac{\partial^2 C(\bm{\beta}; \bm{X}, \bm{y})}{\partial^2 \bm{\beta}}(\bm{\beta} - \bm{\beta}^*)^2 \\
                                  & = C(\bm{\beta}^*; \bm{X}, \bm{y}) + \nabla_{\bm{\beta}} C(\bm{\beta}^*) (\beta - \bm{\beta}^*) + \frac{1}{2} \bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*)^2
\end{align*}
Where $\nabla_{\bm{\beta}} C$ is the gradient of $C$ w.r.t. $\bm{\beta}$ and
$\bm{H}_{\bm{\beta}}$ is the Hesse-matrix (second derivative) of $C$ w.r.t.
$\bm{\beta}$. This should be a good approximation for $C$ sufficiently close to
$\bm{\beta}^*$. We then can find the derivative of this w.r.t. $\bm{\beta}$:
\begin{align*}
    \frac{\partial C(\bm{\beta}, \bm{X}, \bm{y})}{\partial \bm{\beta}} & \approx
    \nabla_{\bm{\beta}} C(\bm{\beta}^*) + \bm{H}_{\bm{\beta}}(\bm{\beta}^*)(\bm{\beta} - \bm{\beta}^*) \\
\end{align*}
If we set this to $0$ we further get:
$$\bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) = -\nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
which leads to
$$\bm{\beta} - \bm{\beta}^* = - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
and finally
$$\bm{\beta} = \bm{\beta}^* - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$

This tells us that the minimum of the cost-function, using any $\bm{\beta}^*$ is
given as above. The problem with this is that this is using the
Taylor-expansion, which is only reliable close to $\beta^*$ so chances are that
using this formula will not give us a global minimum. However it generally tends
to bring us closer to the minimum we are looking after. What we then usually do
is to start with some initial guess for the optimal $\bm{\beta}$, which we
denote $\bm{\beta}^{(0)}$. We then for $k=1, \dots$ let $\bm{\beta}^{(k)} =
    \bm{\beta}^{(k-1)} - \bm{H}_{\bm{\beta}}(\bm{\beta}^{(k-1)})^{-1}
    \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)})$ all the way until we have convergence.

\subsection{Learning-rate schedulers}
Something which often is problematic when using the Newton-Raphson method as
described over is that we need to estimate the Hessian matrix and also invert
it. First of all finding a way to calculate the second derivative can be a
difficulty in and of itself, but it may also be computationally expensive to do
so. Not only that, we also need to invert this matrix which can also become very
computationally expensive, and perhaps also numerically unstable with higher
dimensional hesse matrices (remember that the Hessian matrix is a square matrix
with the same dimensions as $\bm{\beta}$ in both width and height).  Inverting
this matrix then may not be a problem if we only have a few parameters, but for
example when dealing with logistic regression with lots of features, or neural
networks with lots of hidden nodes and layers this becomes unviable. We
therefore will use various methods to approximate this hessian matrix.

\subsubsection{Ordinary gradient descent}
Ordinary gradient descent is a very simple way of approximating the inverse of
the Hessian matrix. Here we simply approximate it with some numerical
learning-rate, which we denote $\lambda$. This way our algorithm becomes:
$$\bm{\beta}^(k) = \bm{\beta}^{(k-1)} - \lambda \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)})$$
Often we may just keep this fixed, but we can also make it depend on $k$. For
example one may wish to start with a higher learning-rate and make this decrease
with the number of iterations. This method I will refer to as using time-decay.
When using gradient descent for training our model $\lambda$ then becomes a
hyperparameter in our model. Often this hyperparameter is rather important in
determining the fit of the model. We often want a rather high learning-rate as
this will lead to faster convergence towards some optimum, however if we make it
to big we may not get convergence at all. I will mainly throughout the code
simply set up a grid of learning-rates and fit to all of them, and simply choose
the learning-rate which gives the best performance, but there are countless
other more advanced way of tuning this parameter.

\subsection{Momentum}
One problem with gradient descent which one quickly sees is that is can be quite
slow to learn. Ideally we would like the model to minimize the cost/loss as
quick as possible, and therefore one might want to use methods designed to
accelerate this learning process. One relatively simple such method is using a
momentum, which often accelrates learning quite drastically especially in cases
with high curvatures and small consistent or noisy gradients
\cite[s.~8.3.2]{goodfellow2016deep}. This method introduces a variable $\bm{v}$
which describes the direction and speed which the parameter $\theta$ is moving
in parameter space \cite[s.~8.3.2]{goodfellow2016deep}, much like the role of
velocity in physics. To calculate this $\bm{v}$ we need a hyperparameter $\alpha
    \in [0, 1)$ and we initialize $\bm{v} = \bm{0}$. Then for each
learning-iteration we update $\bm{v}$ the following way:
$$\bm{v} = \alpha \bm{v}_{prev} - \eta \Delta_{\bm{\theta}} C(\bm{x}, \bm{y}, \bm{\theta})$$
We then can simply update the $\bm{\theta}$ as follows:
$$\bm{\theta} = \bm{\theta}_{prev} + \bm{v}$$
We then see that the value of the $\bm{v}$ will depend on the current cost-gradient
w.r.t. $\bm{\theta}$ as well as the previous $\bm{v}$, which again depends on
the previous cost-gradient w.r.t. $\bm{\theta}$ and the previous previous
$\bm{v}$ and so on. Therefore we see that $\bm{v}$ is influenced by all the
previous cost-gradients, weighted differently, in contrast to just using the
current cost-gradient, and granted they point in a somewhat similar direction,
the $\bm{v}$ will grow bigger and bigger in size. It is therefore not that
difficult to see that this can give quicker learning.


Keep in mind that in theory there is nothing stopping us from combining momentum
with any of the other methods we discuss here, however methods like adam already
incorporates a form of momentum \cite[s.~8.5.3]{goodfellow2016deep}, so we might
not get much quicker learning by using it, if any at all.

\subsubsection{AdaGrad}
AdaGrad is another way designed to give faster learning. This is an algorithm
which uses adaptive learning-rates. These have a difference in that each axis of
the parameter-space $\bm{\theta}$ essentially get a separate learning-rate
\cite[s.~8.5]{goodfellow2016deep}, while these separate learning-rates change/adapt
throughout the learning process, which is a different approach to using momentum.
AdaGrad is a rather simple way of achieving a good such adaptive learning-rate.
It scales the parameters with with big updates, and scales up the ones with
small corresponding updates up. For AdaGrad we introduce one variable $\bm{r}$
which we initially set to $\bm{0}$, as well as a small constant $\delta$ which
we typically set to $10^{-7}$. The algorithms is as follows until the
stopping-criterion is met:
\begin{itemize}
    \item Sample a minibatch of $m$ observations from the training-set
          consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
          variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
    \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
          this minibatch.
    \item Update the $\bm{r}$ to become $\bm{r}_{prev} + \bm{g} \odot \bm{g}$
          ($\odot$ being the Hadamard-product)
    \item Calculate $\Delta \bm{\theta} = -\eta
              \frac{1}{\delta + \sqrt{\bm{r}}} \odot \bm{g}$ and update
          $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
              \bm{\theta}_{prev} + \Delta \bm{\theta}$).

\end{itemize}

\subsubsection{RMSProp}
RMSProp is much like AdaGrad, but is designed to be more robust for
cost-functions with very complex structures like neural nets. Unlike AdaGrad
where we shrink the learning-rates using the sizes of all the previous partial
derivatives of the cost-function for all the axis RMSProp focuses less on the
history of gradients from very long ago, but otherwise using much of the same
approach. The RMSProp algorithm requires therefore a decay-rate $\rho$ as a
hyperparameter as well, along the $\bm{r}$ which we again initialize to be
$\bm{0}$, as well as a small constant $\delta$, which typically is set to
$10^{-6}$. Then we do the following until the stopping-criterion is met:
\begin{itemize}
    \item Sample a minibatch of $m$ observations from the training-set
          consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
          variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
    \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
          this minibatch.
    \item Update the $\bm{r}$ to become $\rho \bm{r}_{prev} + (1 - \rho) \bm{g} \odot \bm{g}$
          ($\odot$ being the Hadamard-product)
    \item Calculate $\Delta \bm{\theta} = -\eta \frac{1}{\sqrt{\delta + \bm{r}}}
              \odot \bm{g}$ and update $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
              \bm{\theta}_{prev} + \Delta \bm{\theta}$).
\end{itemize}

\subsubsection{Adam}
Adam is a very popular learning-rate scheduler, which is probably the method we
will tackle which most often gives the best results. Our implementation is
essentially as described in algorithm 8.7 in \textit{Deep Learning}
\cite[s.~8.6.1]{goodfellow2016deep}. We start the algorithm by initializing the
parameter $\bm{\theta}$ which we would like to optimize the cost/loss w.r.t.
(typically we initialize this by drawing it randomly from for example the normal
distribution which is what we have done in this project). We also set beforehand
$\bm{s} = \bm{0}$, $\bm{r} = \bm{0}$ and $t=0$. Here the algorithms is as
follows until we meet the stopping criterion:
\begin{itemize}
    \item Sample a minibatch of $m$ observations from the training-set
          consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
          variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
    \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
          this minibatch.
    \item Increment $t$ by $1$ (i.e. set $t=t_{prev}+1$).
    \item Update $\bm{s}$ to become $\rho_1 \bm{s}_{prev} + (1 - \rho_1)\bm{g}$.
    \item Update $\bm{r}$ to become $\rho_2 \bm{r}_{prev} + (1 - \rho_2)g \odot
              g$ ($\odot$ again being the Hadamard-product).
    \item Correct the bias in the first moment by: $\hat{\bm{s}} = \frac{\bm{s}}{1 - \rho_1^t}$.
    \item Correct the bias in the second moment by: $\hat{\bm{r}} = \frac{\bm{r}}{1 - \rho_2^t}$.
    \item Finally calculate $\Delta \bm{\theta} = -\eta
              \frac{\hat{\bm{s}}}{\sqrt{\hat{\bm{r}}} + \delta}$ and update $\bm{\theta}$
          accordingly (i.e. set $\bm{\theta} = \bm{\theta}_{prev} + \Delta
              \bm{\theta}$).
\end{itemize}
Where $\eta$ is the step-size (or learning-rate as we will call it), $\rho_1$
and $\rho_2$ are the decay-rate which must be in $[0, 1)$ (typically we set the
defaults to $\rho_1 = 0.9$ and $\rho_2 = 0.999$), $\delta$ is just a small
constant (we set it to $10^{-8}$). Note that we can in theory drop the minibatch
sampling and still get a functioning algorithm, but this will often give much
worse results.

\subsection{Automatic differentiation}

\subsection{Neural networks}
There are many different implementations of neural networks out there. We will
in this project focus on one of the simplest ones to implement, namely the
multi-layer-perceptron.

\subsubsection{Forward propagation algorithm}
The forward propagation algorithm is rather simple.

\subsubsection{Backward propagation algorithm}

\section{Results}

\bibliography{./sources.bib}

\end{document}
\documentclass{article}
\title{Project 2 \\ FYS-STK3155/4155}
\author{Magnus Bergkvam}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}


\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
In this project we tested out performances of neural networks for regression and
binary classification purposes, and compared this with the linear models we
explored in the previous project \cite{githubrepoproject1} (in the case of
regression), as well as our own implementation of logistic regression (for
classification). We here implemented our own neural network code from scratch,
basing the training on the backpropagation-algorithm.  With gradient
optimization being a central part of the development of both neural networks and
logistic regression we also dove into various ways of optimizing the cost
function through the Newton-Raphson method, in particular. We also explored what
effect the various hyperparameters introduced in the methods have on how quick we
are able to optimize the cost, and the performance of the final model. In
particular we explored the effect of the learning-rate and
regularization-parameter, and we looked at various methods for optimizing
through the Newton-Raphson method, including ordinary gradient descent,
stochastic gradient descent, adagrad, rmsprop and adam. For the regression we
used the franke-function which we introduced in project 1
\cite{githubrepoproject1}, while for classification we used breast cancer data
from wisconsin \cite{sklearncancerdata} \cite{breastcancerwisconsin}. What we
saw was that for the franke-function with regularization we did not get any
major performance gains compared to the simpler linear models we tested in
project 1 \cite{githubrepoproject1}. For the cancer data we also saw that while
the neural networks did perform a little better than logistic regression,
however the logistic regression also managed to give us very good results.
Throughout the project we have manifested again that just because neural
networks might be very powerful and impressive in certain tasks, there are still
cases where simpler models will outperform them. We also saw which parts of a
neural net was most instrumental for good model performance, with especially the
learning-rate, regularization parameter, the learning-rate scheduler, and the
amount of epochs being the most instrumental for cost-minimization, while
activation functions and hidden layer sizes didn't matter all that much.

\section{Introduction}
In the last $10$ or so years, neural networks have really exploded in
popularity. Various different types of neural networks have proved effective for
various different applications all from speech-recognition, to image processing,
to complex language tasks.

\section{Methods}
\subsection{The Newton-Raphson method}
Very central to the numerical optimization we do is the Newton-Raphson method.
This is a method which allows us to find the minimum of some function by using
the gradient its gradient. The method has it's origins in the Taylor-expansion
of the function we wish to minimize. In our case we use this to minimize
cost-functions for the models we use. Let $C(\bm{\beta}; \bm{X}, \bm{y})$ be
some general cost-function depending on some design matrix $\bm{X}$ and a
response vector $\bm{y}$. We wish to find the $\bm{\beta}$ which minimizes this
cost, given $\bm{X}$ and $\bm{y}$, i.e. we minimize $C$ with respect to
$\bm{\beta}$, while keeping $\bm{X}$ and $\bm{y}$ fixed. We can then approximate
the cost using a Taylor-expansion of second order around some value
$\bm{\beta}^*$, in which case we get:
\begin{align*}
      C(\bm{\beta}, \bm{X}, \bm{y}) & \approx C(\bm{\beta}^*; \bm{X}, \bm{y}) + \frac{\partial C(\bm{\beta}^*; \bm{X}, \bm{y})}{\partial \beta}(\beta - \bm{\beta}^*) + \frac{1}{2} \frac{\partial^2 C(\bm{\beta}; \bm{X}, \bm{y})}{\partial^2 \bm{\beta}}(\bm{\beta} - \bm{\beta}^*)^2 \\
                                    & = C(\bm{\beta}^*; \bm{X}, \bm{y}) + \nabla_{\bm{\beta}} C(\bm{\beta}^*) (\beta - \bm{\beta}^*) + \frac{1}{2} \bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*)^2
\end{align*}
Where $\nabla_{\bm{\beta}} C$ is the gradient of $C$ w.r.t. $\bm{\beta}$ and
$\bm{H}_{\bm{\beta}}$ is the Hesse-matrix (second derivative) of $C$ w.r.t.
$\bm{\beta}$. This should be a good approximation for $C$ sufficiently close to
$\bm{\beta}^*$. We then can find the derivative of this w.r.t. $\bm{\beta}$:
\begin{align*}
      \frac{\partial C(\bm{\beta}, \bm{X}, \bm{y})}{\partial \bm{\beta}} & \approx
      \nabla_{\bm{\beta}} C(\bm{\beta}^*) + \bm{H}_{\bm{\beta}}(\bm{\beta}^*)(\bm{\beta} - \bm{\beta}^*) \\
\end{align*}
If we set this to $0$ we further get:
$$\bm{H}_{\bm{\beta}}(\bm{\beta}^*) (\bm{\beta} - \bm{\beta}^*) = -\nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
which leads to
$$\bm{\beta} - \bm{\beta}^* = - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$
and finally
$$\bm{\beta} = \bm{\beta}^* - \bm{H}_{\bm{\beta}}(\bm{\beta}^*)^{-1} \nabla_{\bm{\beta}} C(\bm{\beta}^*)$$

This tells us that the minimum of the cost-function, using any $\bm{\beta}^*$ is
given as above. The problem with this is that this is using the
Taylor-expansion, which is only reliable close to $\beta^*$ so chances are that
using this formula will not give us a global minimum. However it generally tends
to bring us closer to the minimum we are looking after. What we then usually do
is to start with some initial guess for the optimal $\bm{\beta}$, which we
denote $\bm{\beta}^{(0)}$. We then for $k=1, \dots$ let $\bm{\beta}^{(k)} =
      \bm{\beta}^{(k-1)} - \bm{H}_{\bm{\beta}}(\bm{\beta}^{(k-1)})^{-1}
      \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)})$ all the way until we have convergence.

\subsection{Learning-rate schedulers}
Something which often is problematic when using the Newton-Raphson method as
described over is that we need to estimate the Hessian matrix and also invert
it. First of all finding a way to calculate the second derivative can be a
difficulty in and of itself, but it may also be computationally expensive to do
so. Not only that, we also need to invert this matrix which can also become very
computationally expensive, and perhaps also numerically unstable with higher
dimensional hesse matrices (remember that the Hessian matrix is a square matrix
with the same dimensions as $\bm{\beta}$ in both width and height).  Inverting
this matrix then may not be a problem if we only have a few parameters, but for
example when dealing with logistic regression with lots of features, or neural
networks with lots of hidden nodes and layers this becomes unviable. We
therefore will use various methods to approximate this hessian matrix.

\subsubsection{Ordinary gradient descent}
Ordinary gradient descent is a very simple way of approximating the inverse of
the Hessian matrix. Here we simply approximate it with some numerical
learning-rate, which we denote $\lambda$. This way our algorithm becomes:
$$\bm{\beta}^(k) = \bm{\beta}^{(k-1)} - \lambda \nabla_{\bm{\beta}} C(\bm{\beta}^{(k-1)})$$
Often we may just keep this fixed, but we can also make it depend on $k$. For
example one may wish to start with a higher learning-rate and make this decrease
with the number of iterations. This method I will refer to as using time-decay.
When using gradient descent for training our model $\lambda$ then becomes a
hyperparameter in our model. Often this hyperparameter is rather important in
determining the fit of the model. We often want a rather high learning-rate as
this will lead to faster convergence towards some optimum, however if we make it
to big we may not get convergence at all. I will mainly throughout the code
simply set up a grid of learning-rates and fit to all of them, and simply choose
the learning-rate which gives the best performance, but there are countless
other more advanced way of tuning this parameter.

\subsection{Momentum}
One problem with gradient descent which one quickly sees is that is can be quite
slow to learn. Ideally we would like the model to minimize the cost/loss as
quick as possible, and therefore one might want to use methods designed to
accelerate this learning process. One relatively simple such method is using a
momentum, which often accelrates learning quite drastically especially in cases
with high curvatures and small consistent or noisy gradients
\cite[s.~8.3.2]{goodfellow2016deep}. This method introduces a variable $\bm{v}$
which describes the direction and speed which the parameter $\theta$ is moving
in parameter space \cite[s.~8.3.2]{goodfellow2016deep}, much like the role of
velocity in physics. To calculate this $\bm{v}$ we need a hyperparameter $\alpha
      \in [0, 1)$ and we initialize $\bm{v} = \bm{0}$. Then for each
learning-iteration we update $\bm{v}$ the following way:
$$\bm{v} = \alpha \bm{v}_{prev} - \eta \Delta_{\bm{\theta}} C(\bm{x}, \bm{y}, \bm{\theta})$$
We then can simply update the $\bm{\theta}$ as follows:
$$\bm{\theta} = \bm{\theta}_{prev} + \bm{v}$$
We then see that the value of the $\bm{v}$ will depend on the current cost-gradient
w.r.t. $\bm{\theta}$ as well as the previous $\bm{v}$, which again depends on
the previous cost-gradient w.r.t. $\bm{\theta}$ and the previous previous
$\bm{v}$ and so on. Therefore we see that $\bm{v}$ is influenced by all the
previous cost-gradients, weighted differently, in contrast to just using the
current cost-gradient, and granted they point in a somewhat similar direction,
the $\bm{v}$ will grow bigger and bigger in size. It is therefore not that
difficult to see that this can give quicker learning.


Keep in mind that in theory there is nothing stopping us from combining momentum
with any of the other methods we discuss here, however methods like adam already
incorporates a form of momentum \cite[s.~8.5.3]{goodfellow2016deep}, so we might
not get much quicker learning by using it, if any at all.

\subsubsection{AdaGrad}
AdaGrad is another way designed to give faster learning. This is an algorithm
which uses adaptive learning-rates. These have a difference in that each axis of
the parameter-space $\bm{\theta}$ essentially get a separate learning-rate
\cite[s.~8.5]{goodfellow2016deep}, while these separate learning-rates change/adapt
throughout the learning process, which is a different approach to using momentum.
AdaGrad is a rather simple way of achieving a good such adaptive learning-rate.
It scales the parameters with with big updates, and scales up the ones with
small corresponding updates up. For AdaGrad we introduce one variable $\bm{r}$
which we initially set to $\bm{0}$, as well as a small constant $\delta$ which
we typically set to $10^{-7}$. The algorithms is as follows until the
stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\bm{r}_{prev} + \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta
                  \frac{1}{\delta + \sqrt{\bm{r}}} \odot \bm{g}$ and update
            $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).

\end{itemize}

\subsubsection{RMSProp}
RMSProp is much like AdaGrad, but is designed to be more robust for
cost-functions with very complex structures like neural nets. Unlike AdaGrad
where we shrink the learning-rates using the sizes of all the previous partial
derivatives of the cost-function for all the axis RMSProp focuses less on the
history of gradients from very long ago, but otherwise using much of the same
approach. The RMSProp algorithm requires therefore a decay-rate $\rho$ as a
hyperparameter as well, along the $\bm{r}$ which we again initialize to be
$\bm{0}$, as well as a small constant $\delta$, which typically is set to
$10^{-6}$. Then we do the following until the stopping-criterion is met:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Update the $\bm{r}$ to become $\rho \bm{r}_{prev} + (1 - \rho) \bm{g} \odot \bm{g}$
            ($\odot$ being the Hadamard-product)
      \item Calculate $\Delta \bm{\theta} = -\eta \frac{1}{\sqrt{\delta + \bm{r}}}
                  \odot \bm{g}$ and update $\bm{\theta}$ accordingly (i.e. set $\bm{\theta} =
                  \bm{\theta}_{prev} + \Delta \bm{\theta}$).
\end{itemize}

\subsubsection{Adam}
Adam is a very popular learning-rate scheduler, which is probably the method we
will tackle which most often gives the best results. Our implementation is
essentially as described in algorithm 8.7 in \textit{Deep Learning}
\cite[s.~8.6.1]{goodfellow2016deep}. We start the algorithm by initializing the
parameter $\bm{\theta}$ which we would like to optimize the cost/loss w.r.t.
(typically we initialize this by drawing it randomly from for example the normal
distribution which is what we have done in this project). We also set beforehand
$\bm{s} = \bm{0}$, $\bm{r} = \bm{0}$ and $t=0$. Here the algorithms is as
follows until we meet the stopping criterion:
\begin{itemize}
      \item Sample a minibatch of $m$ observations from the training-set
            consisting of $\{\bm{x}^{(1)}, \dots, \bm{x}^{(m)}\}$ as the explanatory
            variables and $\{\bm{y}^{(1)}, \dots, \bm{y}^{m}\}$.
      \item Calculate the gradient $\bm{g}$ of the cost w.r.t. $\bm{\theta}$ using
            this minibatch.
      \item Increment $t$ by $1$ (i.e. set $t=t_{prev}+1$).
      \item Update $\bm{s}$ to become $\rho_1 \bm{s}_{prev} + (1 - \rho_1)\bm{g}$.
      \item Update $\bm{r}$ to become $\rho_2 \bm{r}_{prev} + (1 - \rho_2)g \odot
                  g$ ($\odot$ again being the Hadamard-product).
      \item Correct the bias in the first moment by: $\hat{\bm{s}} = \frac{\bm{s}}{1 - \rho_1^t}$.
      \item Correct the bias in the second moment by: $\hat{\bm{r}} = \frac{\bm{r}}{1 - \rho_2^t}$.
      \item Finally calculate $\Delta \bm{\theta} = -\eta
                  \frac{\hat{\bm{s}}}{\sqrt{\hat{\bm{r}}} + \delta}$ and update $\bm{\theta}$
            accordingly (i.e. set $\bm{\theta} = \bm{\theta}_{prev} + \Delta
                  \bm{\theta}$).
\end{itemize}
Where $\eta$ is the step-size (or learning-rate as we will call it), $\rho_1$
and $\rho_2$ are the decay-rate which must be in $[0, 1)$ (typically we set the
defaults to $\rho_1 = 0.9$ and $\rho_2 = 0.999$), $\delta$ is just a small
constant (we set it to $10^{-8}$). Note that we can in theory drop the minibatch
sampling and still get a functioning algorithm, but this will often give much
worse results.

\subsection{Automatic differentiation}
What is clear is that in our optimization it will be central to calculate the
gradient of some loss function with respect to some parameter, however it is not
always easy to find analytical gradients by hand. Automatic differentiation is
an algorithmic approach to calculating of functions. This utilizes clever
applications of the chain rule in order to calculate the gradients of functions
without compromising on accuracy (it usually gives almost exactly the same
results as an analytical expression). We will utilize this in some of our
optimizations instead of calculating gradients by hand, and for adding support
in our code so that users don't have to manually calculate gradients by hand,
which can be time-consuming, and a potential for errors in the calculations.

\subsection{Neural networks}
There are many different implementations of neural networks out there. We will
in this project focus on one of the simplest ones to implement, namely the
multi-layer-perceptron. The multi-layer perceptron consists of a input layer
with $p$ nodes (granted we have $p$ covariates), as many hidden layers as we
want, each of which can have also as many nodes as we want, and then an output
layer of the same dimensionality as our response. Linking the layers together is
a set of weights, where each node in the layer before is connected to each node
in the next layer. Additionally each node has it's bias we add in for each node.
This is then sent in to an activation function, which is set for each layer. The
output of this becomes the values (also called activations) of the neurons in
the next layer. Typical choices of activation functions are sigmoid, relu or
leaky relu, which are the ones we will explore in this project. For the final
layer we instead call this the output function. The output function is typically
tailored to whichever data we have. If we for example want to do regression we
often just set this to the identity-function, i.e. we do not transform the
activations at all. In the case where we are doing classification we this choice
is no longer a good one as the identity function is not bounded, and we in that
case want the output to be a probability between $0$ and $1$, so then a
sigmoid-function for example is a good choice.

\subsubsection{Forward propagation algorithm}
With the description of the model done the forward propagation is actually
rather simple. We assume we have $L-1$ hidden layer, with index $L$ being the output layer. Then the following

\subsubsection{Backward propagation algorithm}

\subsubsection{Weight and bias initialization}
We see that at the core of neural networks are these weigh-matrices and bias
vectors, and seeing as we want to use an iterative method (namely the
Newton-Raphson method) it becomes clear that we need to initialize the weights
and biases to something. If we were to initialize all the weights/biases to be
$0$ (or some other constant), all the weights/biases would have the same effect
on the loss, and hence the backward-propagation algorithm. Our solution to this
is to draw the weights randomly from the standard-normal distribution, this way
we will be able to break the unwanted symmetry in our learning-process. For
biases one don't always bother initializing these randomly (although there is
nothing hindering us to do so). What we will do initialize all the biases to the
same value in every layer (we will set then to $0.1$). Setting it to a small
positive value will often activate the neurons when we are using a
relu-function (see \ref{relu-desc}), which again hinders the derivative of the
relu to be $0$, and this enables the network to learn
\cite[s.~6.3.1]{goodfellow2016deep}.

\subsection{Activation functions}
For our experimentation we will use the following three activation functions for
the hidden layers.
\subsubsection{Sigmoid}
The sigmoid activation function limits the activations of the neurons between
$0$ and $1$. The sigmoid activation function is given as the following:
$$\sigma(x) = \frac{1}{1 + \exp(-x)}$$
With the derivative being (this is quite straightforward to calculate):
$$\sigma'(x) = \sigma(x) (1 - \sigma(x))$$
Additionally as mentioned this will also be used as an output-function when we
are doing binary classification, due to the fact that the final neuron
activations will be between $0$ and $1$.

\subsubsection{Relu}
\label{relu-desc}
Another popular activation function we will use is the relu activation function.
Unlike the sigmoid function this does not have a upper bound. Essentially the
idea is to. The function then can be expressed by:
$$\text{relu}(x) = \max(x, 0)$$
The derivative then becomes:
$$\text{relu}'(x) = \begin{cases}
            0 \qquad & \text{if } x < 0 \\
            1        & \text{if } x > 0
      \end{cases}$$
Keep in mind that the derivative is not defined for $x=0$. This however is
usually unproblematic as we almost never get activations being exactly $0$. In
our implementation we will just set the derivative to $0$ if the activation is
exactly $0$ (but again this should more or less never happen).

\subsubsection{Leaky relu}
The leaky relu is a generalization of the relu-function which unlike the
relu-function doesn't completely neutralize negative values.  Technically this
makes this function not bounded below, however with $\delta$ being rather small
severely limits the size of the negative activations, while no such shrinkage is
applied for the positive activations of course. The leaky relu is given by:
$$\text{lrelu}(x) = \begin{cases}
            x \qquad & \text{if } x > 0    \\
            \delta x & \text{if } x \leq 0
      \end{cases}$$
The derivative then becomes:
$$\text{lrelu}'(x) = \begin{cases}
            \delta \qquad & \text{if } x < 0 \\
            1             & \text{if } x > 0
      \end{cases}$$
We see one major difference between this and the relu is that for negative
activations, the derivative of this will not be $0$, but $\delta$. This means
that the network will be able to learn on activations which are negative as
well, albeit likely slower than the positive cases.

\subsection{Cost functions}
In this project we will use mainly two activation functions, namely mse/sse for
regression and cross-entropy for binary-classification. We will now quickly go
into these.

\subsubsection{MSE/SSE}
A natural choice of cost-function when dealing with regression tasks is the MSE.
When the target dimensions are always $1$ (which they are in this project), the
MSE is defined like:
$$\text{mse}(\bm{h}^{(L)}, \bm{t}) = \frac{1}{n} \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = \frac{1}{n}(\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
The derivative with respect to $\bm{h}^{(L)}$ when becomes:
$$\frac{\partial \text{mse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = \frac{2}{n} (\bm{h}^{(L)} - \bm{t})$$
We will also use the total sum of squares (SSE). Minimizing this is the same as
minimizing the MSE, but with one less floating-point operation. The SSE is given
by:
$$\text{sse}(\bm{h}^{(L)}, \bm{t}) = \sum_{i=1}^n (h^{(L)}_i - t_i)^2 = (\bm{h}^{(L)} - \bm{t})^T (\bm{h}^{(L)} - \bm{t})$$
and its derivative then becomes:
$$\frac{\partial \text{sse}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = 2(\bm{h}^{(L)} - \bm{t})$$
The only potential problem with using SSE instead of MSE is that the gradient
will then be dependent on the amount of observations we are using, which can be
problematic with stochastic gradient descent. However in this project we will
set a fixed size of each minibatch, which guarantees that each training-set sent
into this cost/loss function will be of the same size, so this becomes
unproblematic.

\subsubsection{Cross-entropy}
For binary classification we will use cross-entropy as out cost-function.
Minimizing the cross-entropy is equivalent to maximizing the likelihood. We have
the cross-validation given by:
$$\text{cross-entropy}(\bm{h}^{(L)}, \bm{t}) = -\sum_{i=1}^{n} ( t_{i} \log(h^{(L)}_i) + (1 - y) \log(1 - h^{(L)}_i) )$$
With the gradient w.r.t. $h^{(L)}$ then becoming:
$$\frac{\partial \text{cross-entropy}}{\partial \bm{h}^{(L)}}(\bm{h}^{(L)}, \bm{t}) = (\bm{h}^{(L)} - \bm{t}) / ((1 - \bm{h}^{(L)}) \odot \bm{h}^{(L)})$$
Here the division is element-wise. One thing to note is that we see that if one of
our predictions $h^{(L)}_i$ is very close to $0$ while the target value is $1$
or vice versa, which especially can happen in the first iterations, perhaps the
most with some sort of unbounded activation function in the last layer, the
gradient can become very big. If we get too big of a gradient this can be
problematic for many reasons, with one being we can get values too big for the
computer to calculate. In our code we will solve this by clipping the
predictions in the cross-entropy-gradient, so that they don't become too close
to $0$ or $1$ so that this doesn't become a problem. Additionally we will add a
small $\delta$ inside the log of the cross-entropy for the same reason.

\subsubsection{Notation in the code}
Keep in mind that in the code the cost-functions often takes $\hat{y}$ and $y$
instead of $\bm{h}^{(L)}$ and $\bm{t}$ respectively. This essentially means the
same thing, but makes more sense with the notation for usage outside of neural
networks.

\subsection{Model complexities}
One final thing to consider which will be very important for our results is the
model-complexity of neural networks and logistic regression. Logistic regression
requires just as many parameters as we have explanatory variables, so the model
complexity is rather low, at least in the case where we don't have too many
covariates. Neural networks however is very . In both cases we can however introduce regularization which can help
reduce the model complexity.

\subsection{Logistic regression}
The logistic regression model is a commonly used, and rather simple, way to
perform classification tasks. With logistic regression we assume a binary
response $0$ and $1$ typically and model the probability the follwing way:
$$Pr(Y_i = 1 | \bm{x}_i) = \frac{1}{1 + \exp(\bm{x}_i \bm{\beta})} = \sigma(\bm{x}_i \bm{\beta})$$
, where $\sigma$ is obviously the sigmoid-function as already described. We then
can do classification based on this, where we typically classify to $1$ if this
probability is bigger than $0.5$ (this is the boundary we will use here) and $0$
if lower than $0.5$. In order to estimate the $\beta$ parameters we use the
likelihood such a model, given by \cite[s.~4.4]{hastie2009elements}:
$$l(\bm{\beta}) = \sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta})))$$
, where the estimates $\hat{\bm{\beta}}$ become the ones which maximize this
likelihood. The cost-function then becomes:
$$C(\bm{\beta}) = -\sum_{i=1}^{n} (y_i \bm{x}_i \bm{\beta} - log(1 + \exp(\bm{x}_i \bm{\beta}))).$$
To minimize we find the gradient \cite[s.~4.4]{hastie2009elements}:
$$\frac{\partial C(\bm{\beta})}{\partial \bm{\beta}} = -\sum_{i=1}^{n}\bm{x}_i^T(y_i - Pr(Y_i = 1 | \bm{x}_i)) = -\bm{X}^T(\bm{y} - \bm{p}).$$
We then can use the Newton-Raphson method on this in order to estimate the $\bm{beta}$.

\section{Results}

\subsection{General optimization}

\subsection{The franke-function}

\subsection{Wisconsin Cancer data}

\section{Analysis}

\section{Conclusion}

\bibliography{./sources.bib}

\end{document}
\documentclass{article}
\author{Magnus Bergkvam}
\title{Project 1 \\ FYS-STK3155}
\usepackage{url}

\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
We have in this project fitted various models on data generated from the
franke-function, a function which somewhat resembles a landscape, and real-world
geographical data. In both cases we have used polynomial features of
order 5 and fitted models to this franke-function both with and without added
noise.  The models we looked at was ridge, lasso and ordinary least squares.
For both the mean-squared-error and the $R^2$ score we saw that for the
franke-function without noise the ordinary least squares performed the best on
new data, but when we added noise-terms the lasso/ridge performed better.
Throughout the project we have clearly seen that less complex models often
perform better when we have some added noise (at least in the case when we have
somewhat limited data), but that this is not the case when there is no noise (or
a very high signal to noise-ratio). We have also explored how this result is
tied to the bias-variance tradeoff, and manifested the importance this has for
choosing the best model.

\section{Introduction}
Linear regression often is seen as a rather simple machine-learning method in
itself, but we do have less complex models than this again. Choosing the right
model complexity is well known as being an important part in getting the best
possible models for prediction, and in a lot of applications linear regression
offers just the right amount of complexity needed. Especially with cases where
our data is limited, or we have a low signal-to-noise ratio, linear regression
often performs very well \cite{hastie2009elements}. \\

However, when can it become beneficial to choose even less complex models than
this again? This is mainly what this project is about. We will mainly base this
project around the franke-function, which is a 2-dimensional function which
somewhat resembles a landscape \cite{franke2ddesc}. On the franke-function we
will fit ordinary least squares linear regression with polynomial features up to
order $5$ (with interactions), and then compare these results (using mainly the
mean squared error and $R^2$ metrics) to ridge and lasso regression on the
same data. We will also explore the performances of these models on some
real-world geographical data as well. For the franke-function we will be
generating two vectors of random points in $\left[ 0, 1 \right]$, and use
polynomial features with interactions of order $5$ for our data, for all our
models. We will also try to tie the results we get to the well-known
bias-variance tradeoff in machine learning, and on our way there explore various
resampling techniques for getting better estimates for model performances.
\\

We now get into a description of the models we have used, before we present the
results and finally conclude what we have found out.

\section{Methods}
\subsection{Data/Data scaling}
The data and design matrix used thorughout the project is mostly the same for
the different models. The main data we have used here is the franke-function
where the explanatory variables are the two inputs to the franke-function drawn
randomly from $\left[ 0, 1 \right]$ and response as the franke-function of
these. When we added noise we have drawn this from the normal distribution with
mean $0$ and standard-deviance $0.1$ in order to not have too low of a
signal-to-noise-ratio (the franke-function takes quite low values). For the
geographical data we load the data in from a tif file of geographical data, but
the setup of the design matrix is exactly the same, with the difference being
that the $x$'s $y$'s and response draw directly from the tif-file. The design
matrix is largely the same for the different models with some minor differences.
For the $5$ th order polynomials we have included in the first column the
first-order values of the $x$, the second column the second-order values of $x$,
and so on. The sixth to $10$ th column then is the first to fifth order terms of
$y$ and then we take $4 \cdot 4 = 16$ columns for interaction terms between the
two variables. A difference here is that for ordinary least squares we have
added an intercept by setting the first column to consist of only $1$.
Throughout the whole of this project we have scaled the features of the matrix,
simply by centering (in other words subtracting the column mean). Another thing
used throughout the project (except when doing cross-validation) is
train-test-splitting. We then split our data into a train set consisting of
$80\%$ of the datapoints, and $20\%$ of the data for testing. I have written a
function which takes hand of most of the data for various tasks called
\textit{generatedata}. The function has the following signature:

\subsection{Ordinary least squares}
We have two vectors $x$ and $y$ $200$ responses each. We create a design-matrix
$X$ based on polynomial features of different orders up to order $5$ with
interaction terms. As said we here add the first column to add a intercept. Then
to find the $\hat{\beta}_{ols} = (X^T_{train} X_{train})^{-1} X^T_{train}
    y_{train}$. We then can make predictions for the training-set by:
$$\hat{y}_{train} = X_{train} \hat{\beta}_{ols}$$
and for the traning set:
$$\hat{y}_{test} = X_{test} \hat{\beta}_{ols}$$

\subsection{Bootstrap}

\subsection{Cross-validation}

\bibliography{./sources.bib}

\end{document}
\documentclass{article}
\author{Magnus Bergkvam}
\title{Project 1 \\ FYS-STK3155}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[section]{placeins}
\graphicspath{{../figures/}}

% Julia code configuration
\usepackage[usenames,dvipsnames]{color} % more flexible names for syntax highlighting colors
\usepackage{listings}
\lstdefinelanguage{julia}
{
  keywordsprefix=\@,
  morekeywords={
    exit,whos,edit,load,is,isa,isequal,typeof,tuple,ntuple,uid,hash,finalizer,convert,promote,
    subtype,typemin,typemax,realmin,realmax,sizeof,eps,promote_type,method_exists,applicable,
    invoke,dlopen,dlsym,system,error,throw,assert,new,Inf,Nan,pi,im,begin,while,for,in,return,
    break,continue,macro,quote,let,if,elseif,else,try,catch,end,bitstype,ccall,do,using,module,
    import,export,importall,baremodule,immutable,local,global,const,Bool,Int,Int8,Int16,Int32,
    Int64,Uint,Uint8,Uint16,Uint32,Uint64,Float32,Float64,Complex64,Complex128,Any,Nothing,None,
    function,type,typealias,abstract
  },
  sensitive=true,
  morecomment=[l]{\#},
  morestring=[b]',
  morestring=[b]" 
}
\lstset{
basicstyle=\ttfamily, 
columns=fullflexible, % make sure to use fixed-width font, CM typewriter is NOT fixed width
numbers=left, 
numberstyle=\small\ttfamily\color{Gray},
stepnumber=1,              
numbersep=10pt, 
numberfirstline=true, 
numberblanklines=true, 
tabsize=4,
lineskip=-1.5pt,
extendedchars=true,
breaklines=true,        
keywordstyle=\color{Blue}\bfseries,
identifierstyle=, % using emph or index keywords
commentstyle=\sffamily\color{OliveGreen},
stringstyle=\color{Maroon},
showstringspaces=false,
showtabs=false,
upquote=false,
texcl=true % interpet comments as LaTeX
}
\lstset{
literate={σ}{{$\sigma$}}1
{λ}{{$\lambda$}}1
}
% End julia code

\begin{document}
\maketitle
\bibliographystyle{unsrt}

\section{Abstract}
We have in this project fitted various models on data generated from the
franke-function, a function which somewhat resembles a landscape, and real-world
geographical data. In both cases we have used polynomial features of
order 5 and fitted models to this franke-function both with and without added
noise.  The models we looked at was ridge, lasso and ordinary least squares.
For both the mean-squared-error and the $R^2$ score we saw that for the
franke-function without noise the ordinary least squares performed the best on
new data, but when we added noise-terms the lasso/ridge performed better.
Throughout the project we have clearly seen that less complex models often
perform better when we have some added noise (at least in the case when we have
somewhat limited data), but that this is not the case when there is no noise (or
a very high signal to noise-ratio). We have also explored how this result is
tied to the bias-variance tradeoff, and manifested the importance this has for
choosing the best model.

\section{Introduction}
Linear regression often is seen as a rather simple machine-learning method in
itself, but we do have less complex models than this again. Choosing the right
model complexity is well known as being an important part in getting the best
possible models for prediction, and in a lot of applications linear regression
offers just the right amount of complexity needed. Especially with cases where
our data is limited, or we have a low signal-to-noise ratio, linear regression
often performs very well \cite[p.~43]{hastie2009elements}. \\

However, when can it become beneficial to choose even less complex models than
this again? This is mainly what this project is about. We will mainly base this
project around the franke-function, which is a 2-dimensional function which
somewhat resembles a landscape \cite{franke2ddesc}. On the franke-function we
will fit ordinary least squares linear regression with polynomial features up to
order $5$ (with interactions), and then compare these results (using mainly the
mean squared error and $R^2$ metrics) to ridge and lasso regression on the
same data. We will also explore the performances of these models on some
real-world geographical data as well. For the franke-function we will be
generating two vectors of random points in $\left[ 0, 1 \right]$, and use
polynomial features with interactions of order $5$ for our data, for all our
models. We will also try to tie the results we get to the well-known
bias-variance tradeoff in machine learning, and on our way there explore various
resampling techniques for getting better estimates for model performances.
\\

We now get into a description of the models we have used, before we present the
results and finally conclude what we have found out.

\section{Methods}
\subsection{The franke function}
The franke-function is a function of two variables with the following definition:
\begin{align*}
    f(x,y) & = \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
           & +\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }
\end{align*}
We will use this function to generate a response from two explanatory variables
$x1$ and $x2$ both of which will be drawn randomly from $\left[ 0, 1 \right]$.
To get a visual picture of how this response looks, given some $x1$ and $x2$
values on $\left[ 0, 1 \right]$ we have the following plot \cite[figures,
    frankefunction.png]{githubrepoproject1}:

\begin{figure}
    \includegraphics[scale=0.5]{frankefunction}
    \caption{A surface plot of the franke-function on $\left[ 0, 1 \right] \times \left[ 0, 1 \right]$}
    \label{franke-function-plot}
\end{figure}

As we can see this does somewhat resemble a terrain. We will generate this data
both with and without noise.  In the case of without noise we will simply get
the following response:
$$\mathbf{y} = f(\mathbf{x1}, \mathbf{x2})$$
(Here where $f$ is applied element-wise to $\mathbf{x1}$ and $\mathbf{x2}$).
With noise we simply get:
$$\mathbf{y} = f(\mathbf{x1}, \mathbf{x2}) + \mathbf{\epsilon}$$
Where in this case $\epsilon_i \sim N(0, \sigma^2)$. We will use $\sigma = 0.1$
throughout all of our calculations (but this is easily adjustable in the code).

\subsection{Geographical data}
As mentioned, another source for data which was used for analysis of the
different models is some real-world geographical data. The data is stored as a
tiff image file and consists simply of a set of numerical values in a square
grid, where the numerical value indicates the height of the terrain in some way.
The data is gathered from \url{https://earthexplorer.usgs.gov/}, and we have
here downloaded a region around \textit{Møsvatn Austfjell} in Norway. This data
includes a lot of datapoints ($3601\times 1801 = 6485401$ to be exact), which is
quite a lot more than we need to fit a good model. Trying to fit a lot of
different models to such a big amount of data will often take a lot of memory
and processing power, so for this sake I have throughout our calculations
limited the data to look at only the first $400\times 500 = 20000$ observations,
which should be more than enough for our case. When reading the tiff-file we
only get a matrix of values out, but we will look at this as an image, and then
use the position of the response as explanatory variables for our model. In
other words, if say the fifth pixel in the fourth row has the value $10^{-5}$,
the response will obviously be $10^{-5}$, while the two explanatory variables
will have values $5$ and $4$.  This way we will be able to gather data whose
structure closely resembles that of the franke-function. We will use thus us the
same techniques to setup design a matrix and response, as we did with the
franke-function.

\subsection{Data/Data scaling}
The data and design matrix used throughout the project is mostly the same for
the different models, at least in the case of model evaluation for the
franke-function. The data we have used here is simply a collection of two
randomly generated datapoint-vectors $x1$ and $x2$, each of size $200$, and the
response is simply the value of the franke-function evaluated at some point
(plus some added noise in some cases). When we added noise we have drawn this
from the normal distribution with mean $0$ and standard-deviance $0.1$ in order
to not have too low of a signal-to-noise-ratio (the franke-function gives quite
low values). For the geographical data we load the data in from a tiff file of
geographical data, but the setup of the design matrix is exactly the same, with
the difference being that the $x1$'s $x2$'s and response have been drawn directly from the
tiff-file. The design matrix is largely the same for the different models with
some minor differences.  For the $5$th order polynomials we have included in
the first column the first-order values of the $x$, the second column the
second-order values of $x$, and so on. The sixth to $10$ th column then is the
first to fifth order terms of $y$ and then we take $4 \cdot 4 = 16$ columns for
interaction terms between the two variables. A difference here is that for
ordinary least squares we have added an intercept by setting the first column to
consist of only $1$.
\\
Throughout the whole of this project we have scaled the
features of the matrix, simply by centering the mean around $0$ and scaling by
the standard deviance. In the case of ordinary least squared regression this
scaling is not very important for model performance, as the model will be able
to scale the $\beta$'s correspondingly for different scaled data, but in the
case of ridge/lasso which both include a penalty term (see the ridge/lasso
sections), it becomes important to scale the data so that the scale of the data
doesn't affect how much we penalize the parameters (I will go into more detail
on this in the ridge/lasso sections).
\\
A last thing of note we have used throughout the project is train-test-splitting
(except when doing cross-validation). We then split our data into a train set
consisting of $80\%$ of the datapoints, and $20\%$ of the data for testing.
This lets us evaluate how well our model performs on "new" data, i.e. data the
model has not been trained on from the dataset. The reason this is important is
in order for us to recognize overfitted models, which often become the case when
we have a higher model complexity \cite[s.~7.2]{hastie2009elements}. In such
cases we could have a model that predicts the training data very well, but
generalizes very poorly to new data.  For the franke-function I have written a
function \textit{generatedata} which takes hand of all of the mentioned above
(here I am using Julia). It is a bit long, but well worth inclusion here as it
is central to all of the programs that use the franke-function:
\begin{lstlisting}[language=julia]
# Generate a design matrix consisting of random x-s (two explanatory variables)
# with a polynomial of a given order (we will use order 5 for our analysis
# mainly). Additionally includes options for adding intercept column, noise,
# number of observations and random seed.
function generatedata(order::Int64; split=true, include_intercept=false, add_noise=false, noise_σ=0.1, n=200, custom_seed=1000)
    # Setting the seed for train test splitting and random x1/x2
    seed!(custom_seed)

    # Generating x-s
    x1 = rand(n)
    x2 = rand(n)

    # Creating the design matrix
    X = generatedesignmatrix(x1, x2, order)

    # Creating the response
    y = Functions.frankefunction(x1, x2)


    # Shuffle before train-test splitting
    Xs, ys = shufflematrices(X, y)

    # Train-test splitting
    if !split
        return standardscale(Xs, Xs), ys
    end
    indextosplitat = convert(Int, floor(size(Xs, 1) * 0.8))
    X_train, X_test = Xs[1:indextosplitat, :], Xs[(indextosplitat+1):size(Xs, 1), :]
    y_train, y_test = ys[1:indextosplitat, :], ys[(indextosplitat+1):size(ys, 1), :]

    # Scaling X\_train/X\_test
    # Here we create a copy of the X\_train matrix as we want to use the
    # column-means of this to scale both the X\_train and X\_test columns.
    X_train_original = copy(X_train)
    X_train = standardscale(X_train, X_train_original)
    # We use the column means from the original X\_train to subtract from the
    # columns in X\_test.
    X_test = standardscale(X_test, X_train_original)

    if add_noise
        # Add response with noise
        y_train += rand(Normal(0, noise_σ), length(y_train))
        y_test += rand(Normal(0, noise_σ), length(y_test))
    end

    if include_intercept
        X_train = [ones(size(X_train, 1)) X_train]
        X_test = [ones(size(X_test, 1)) X_test]
    end

    return X_train, X_test, y_train, y_test
end
\end{lstlisting}

Here we have used some other local functions like standardscale (scaling as
described in the scaling section in this report) and shufflematrices (random
shuffles rows in a matrix), which it should be clear what these do based on
their name, but I'll also include the implementation of the
\textit{generatedesignmatrix}, as it is very central for all the calculations we
have done:
\begin{lstlisting}[language=julia]
# Generate a matrix with features as polynomial terms with interactions
function generatedesignmatrix(x1, x2, order)
    X = zeros((length(x1), 2 * order + (order - 1)^2))
    for i in 1:order
        X[:, i] = x1 .^ i
        X[:, order+i] = x2 .^ i
    end
    for i in 1:(order-1)
        for j in 1:(order-1)
            X[:, 1+order+i*(order-1)+j] = (x1 .^ i) .* (x2 .^ j)
        end
    end

    return X
end
\end{lstlisting}

Shuffling the data is perhaps not nessecary here in the case of the
franke-function seeing as the explanatory variables are randomly drawn (but I
have done so anyway just out of good practice), however in the case of the
landscape data this is very important, as here the data is sorted when reading
the data in, and if we then do not shuffle the data before, and we then apply
train-test-splitting, we will get different regions of our response for the
training and testing data, in other words, the model will be trained on data
from some area and tested on data from another area, which will then probably
give quite poor performance (unless the two regions the train-data and test-data
are gathered from are very similar).


\subsection{Ordinary least squares}
We have two vectors $x$ and $y$ $200$ responses each. We create a design-matrix
$X$ based on polynomial features of different orders up to order $5$ with
interaction terms. As said we here add the first column to add a intercept.
Ordinary least squares regression finds the $\mathbf{\beta}$ which minimizes the
loss-function (this optimal value we call $\hat{\mathbf{\beta}}$) given like:
$$C(\beta) = (y - X\mathbf{\beta})^T (y - X\mathbf{\beta})$$
Then to find the $\hat{\beta}_{ols} = (X^T_{train} X_{train})^{-1} X^T_{train}
    y_{train}$. We then can make predictions for the training-set by:
$$\hat{y}_{train} = X_{train} \hat{\beta}_{ols}$$
and for the traning set:
$$\hat{y}_{test} = X_{test} \hat{\beta}_{ols}$$

\subsection{Ridge and lasso}
Ridge and lasso is in lots of ways very similar to ordinary least squares
regression. The setup of the model is exactly the same, where for both these
models we estimate some parameter vector $\mathbf{\beta}$, where we assume
$$y = X \mathbf{\beta} + \mathbf{\epsilon}$$
The difference arises with the cost function we minimize to estimate the parameters. For ridge we use:
$$C(\mathbf{\beta}) = \left(\sum_{i=1}^{n} (y_i - X_{i *} \beta)\right) + \lambda \sum_{i=1}^p \beta_i^2$$
and for lasso:
$$C(\mathbf{\beta}) = \left(\sum_{i=1}^{n} (y_i - X_{i *} \beta)\right) + \lambda \sum_{i=1}^p \lvert \beta_i \rvert$$
for some hyperparameter $\lambda$, which determines how much we want to
penalize high parameter values. Here we see further why it is important to scale
the data beforehand. If we have data with widely different scaling this will
impact how the optimal corresponding $\beta$ to the feature should be (not
regarding the penalty term). If we have a feature with very high values, this
$\beta$ will be smaller, than if we use a smaller scale/unit. But then this term
$\beta^2$ will be much bigger, and we will reduce the cost much further by
decreasing this parameter than if it had a smaller scale. In order to hinder
this happening it them becomes important to scale the features somewhat alike,
so that the scale the features is given on does not determine the fit we get.

\subsection{Bias and variance of ols/ridge/lasso}
We now calculate some properties/values of the ridge regression. We first
calculate the expectation of some $y_i$. We have:
\begin{align*}
    E(y_i) & = E\left[ (\mathbf{X}\mathbf{\beta})_i + \epsilon_i \right]                 \\
           & = E\left[ (\mathbf{X}\mathbf{\beta})_i \right] + E\left[ \epsilon_i \right] \\
           & = (\mathbf{X}\mathbf{\beta})_i + 0                                          \\
           & = \mathbf{X}_{i, *}\mathbf{\beta}
\end{align*}
For the variance get have:
\begin{align*}
    Var(y_i) & = Var(\mathbf{X}_{i, *} \mathbf{\beta} + \epsilon_i) \\
             & = Var(\epsilon_i)                                    \\
             & = \sigma^2
\end{align*}
(The first to the second line here follows from the fact that $\mathbf{X} \mathbf{\beta}$ is not stochastic).
This is enough to conclude that $y_i \sim N(\mathbf{X}_{i, *} \mathbf{\beta},
    \sigma^2)$.  To analyze our $\mathbf{\hat{\beta}}$ we can look at the
expectance/variance of this. In order to elegantly calculate this we use two
somewhat basic results, which I will first prove. Let $\mathbf{A}$ be a
non-stochastic ($p\times n$) matrix and $\mathbf{X}$ be a stochastic ($n \times
    p$) vector. We then have the following two results.
$$E(\mathbf{A} \mathbf{X}) = \mathbf{A} E(\mathbf{X})$$
and
$$Var(\mathbf{A} \mathbf{X}) = \mathbf{A} Var(\mathbf{X}) \mathbf{A}^T$$
The first one is rather easy to see. We have that $E(\mathbf{A} \mathbf{X}) =
    (E((\mathbf{A} \mathbf{X})_1, \dots, E((\mathbf{A} \mathbf{X})_n))^T$. We have
$$E((\mathbf{A} \mathbf{X})_i) = E(\sum_{j=1}^{p} \mathbf{A}_{i j} \mathbf{X}_j) = \sum_{j=1}^{p} \mathbf{A}_{i j} E(\mathbf{X}_j) = (\mathbf{A} E(\mathbf{X}_i))_i$$
Hence we see that $E(\mathbf{A}\mathbf{X}) = \mathbf{A} E(\mathbf{X})$. For the variance we need another helping result, namely we need to find $E(X A^T)$. We have:
\begin{align*}
    E(X A^T) & = \dots \\
             & = \dots
\end{align*}
Using this we get:
\begin{align*}
    E(\mathbf{\hat{\beta}}) & = E((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y})             \\
                            & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T E(\mathbf{Y})             \\
                            & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{X} \mathbf{\beta} \\
                            & = \mathbf{\beta}
\end{align*}
and
\begin{align*}
    Var(\mathbf{\hat{\beta}}) & = Var((\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y})                                                              \\
                              & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T Var(\mathbf{Y}) \left( (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \right)^T \\
                              & = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \sigma^2 I \mathbf{X} \left( (\mathbf{X}^T \mathbf{X})^{-1} \right)^T        \\
                              & = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{X} \left((\mathbf{X}^T \mathbf{X})^T \right)^{-1}           \\
                              & = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}                           \\
                              & = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{align*}

\subsection{Bootstrap}
Since this project essentially boils down to comparing metrics for different
models on some data it becomes important that these metrics which we calculate
are reliable. In the case of the franke-function we have used $200$ datapoints,
which we have split into a training-set and test-set. We have then trained the
data on the training-set and then evaluated the trained model on the test-set,
however since the test-set is only $20\%$ of our datapoints, we have that this
test-set is rather small. This can make the test-error which we achieve quite
specific to the model which we have trained, and our metrics may not be that
reliable. In order to combat this we will use resampling techniques. One way
which do this by is using bootstrap. We will use bootstrapping to estimate the
MSE and R\^2 scores on the test-data. Thus we will first have to split the data
into a training and testing-set like before. Then we draw bootstrap samples
(i.e. we draw $n$ observations from the $n\times p$ design-matrix $X_{train}$
with replacement) from the training-data and fit the model on this data and
evaluate the metrics on the test-data. We do this $B$ times and take the mean of
these metrics to get our final estimates. This often will give us a better
estimate for the metrics because it does not require assumptions on error terms,
which causes bootstrap to give us more reliable results when we have a smaller
sample size (which we do in this case, especially for) \cite[s.~5.3]{lecturenotes5}.

\subsection{Cross-validation}
K-fold cross-validation is another resampling-technique which also can better our
estimates for the various metrics. Unlike with bootstrapping we don't need to
split the data into a training-set or test-set anymore. K-fold cross-validation
works by splitting the data into $K$ folds of as equal sizes as possible. Now
denote for some $1 \leq i \leq K$ $X^{-i}$ as the design-matrix with fold $i$
excluded, and $X^i$ as the data for fold $i$ of our design-matrix. For
cross-validation we do for $i = 1, \dots, K$:
\begin{enumerate}
    \item Fit our model on $X^{-i}$
    \item Evaluate our metric on $X^{i}$
\end{enumerate}
We then, much like bootstrapping average over all the metrics to get our final
metric. The reason this improves the estimate for the metrics is that since
every fold is used for evaluating the metric, that every observation also is
used in the estimate for our final metric. This is in contrast to our earlier
estimate, which the metric is only evaluated using the rather small test-set.
This should give us a better estimate for the true MSE or R\^2.
\section{Results}
\subsubsection{Franke-function}
\subsubsection{Ordinary least squares}
We have located the code for performing ordinary least squares regression on the
franke-function with polynomial features up to order $5$ in the
\textbf{LinearRegression.jl} on the project repository
\cite{githubrepoproject1}. Running this we can extract the following metrics:\\
\begin{tabular}{| c | c | c | c | c |}
          & \multicolumn{2}{|c|}{Without noise} & \multicolumn{2}{|c|}{With noise}                           \\
          & MSE                                 & R\^2                             & MSE        & R\^2       \\
    TRAIN & $0.001046$                          & $0.985926$                       & $0.008400$ & $0.905406$ \\
    TEST  & $0.007677$                          & $0.907600$                       & $0.058654$ & $0.395051$ \\
\end{tabular}

We see there is quite good performance on the training-set and test-set when
noise is not included. When we include noise-terms the model-performance on the
training data is still quite good, but with the test-data we get rather poor
results.

\subsubsection{Ridge}
If we run the \textbf{Ridge.jl} program in the project repository
\cite{githubrepoproject1}, we can extract the following results:\\
\begin{tabular}{| c | c | c | c | c |}
          & \multicolumn{2}{|c|}{Without noise} & \multicolumn{2}{|c|}{With noise}                           \\
          & MSE                                 & R\^2                             & MSE        & R\^2       \\
    TRAIN & $0.001046$                          & $0.985926$                       & $0.008400$ & $0.905406$ \\
    TEST  & $0.003592$                          & $0.956765$                       & $0.017352$ & $0.821028$ \\
\end{tabular}

With the optimal $\lambda$ for test-data without noise being $1.659587\cdot
    10^{-2}$ and when we include noise it becomes $1.513561$.
Additionally we can see how the MSE and R\^2 score varies with different values
of the hyperparameter $\lambda$ with the following two plots:\\

\begin{figure}[!htb]
    \centerline{\includegraphics[scale=0.5]{ridge_without_noise}}
    \caption{A plot of the $\lambda$ in a ridge-model without noise on log10-scale plotted against the MSE and R\^2. We see that performances of these metrics goes a bit up and down for the different values of $\lambda$.}
    \label{Ridge-with-noise}
\end{figure}

\begin{figure}[!htb]
    \centerline{\includegraphics[scale=0.5]{ridge_with_noise}}
    \caption{A plot of the $\lambda$ in a ridge-model with $\lambda$s on log10-scale plotted against the MSE and R\^2. We see a somewhat similar relationship as for the case with noise, but with greater scales on the y-axis}
    \label{Ridge-no-noise}
\end{figure}

\subsubsection{Lasso}
The \textbf{Lasso.jl} on the project repo \cite{githubrepoproject1} runs lasso
regression on the franke-function data and calculates the MSE and R\^2. Running
this we get:
\begin{tabular}{| c | c | c | c | c |}
          & \multicolumn{2}{|c|}{Without noise} & \multicolumn{2}{|c|}{With noise}                           \\
          & MSE                                 & R\^2                             & MSE        & R\^2       \\
    TRAIN & $0.003280$                          & $0.955866$                       & $0.011319$ & $0.872539$ \\
    TEST  & $0.002737$                          & $0.967054$                       & $0.016451$ & $0.830329$ \\
\end{tabular}

\begin{figure}
    \centerline{\includegraphics[scale=0.5]{lasso_without_noise}}
    \caption{A plot of the $\lambda$ in a lasso-model with $\lambda$s on log10-scale plotted against the MSE and R\^2. Unlike the Ridge-case this completely flattens out at the end. This is due to the fact that for such high values of $\lambda$ the model-parameters which minimize the loss is simply with all the $\beta$s (other than the intercept term) is $0$}
    \label{Lasso-no-noise}
\end{figure}
\begin{figure}
    \centerline{\includegraphics[scale=0.5]{lasso_with_noise}}
    \caption{A plot of the $\lambda$ in a lasso-model with $\lambda$s on log10-scale plotted against the MSE and R\^2. We see the same flattening as in figure \ref{Lasso-no-noise}}
    \label{Lasso-with-noise}
\end{figure}

\subsection{Landscape data}


\section{Analysis}
As we can see from the results, the ordinary least squares performs rather good
on the models without noise, with ordinary least squares regression performing
considerably better than both ridge and lasso, with ridge and lasso having quite
comparable results. Looking at

\section{Conclusion}

\bibliography{./sources.bib}

\end{document}